{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcCAPKl5SJZJ",
    "outputId": "40a0cb88-b8a6-4cbf-f373-307b88043333"
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hs9Wa1kJSBJq",
    "outputId": "ba486871-a306-4936-8914-d9ff97612148"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install sentence-transformers torch pandas scikit-learn joblib requests lightgbm torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7a47abcb578c4c699ff4d26b2b565f2b",
      "77ec5073999a4710b20fa302b0f97ddb",
      "627d03b7aea94f4abae4ba7fee2108b7",
      "5cb7bd38a61d4769b523ee380cada3f5",
      "e38cef5b32ce4b1f9278e03da2e324fb",
      "03f7a97bd6e24c22afd2389b2aae10db",
      "022740097547414998295ab1a8326569",
      "e1651c798ca446a58e296fb3528a0482",
      "b4d20bc9b93c4a4da9d6e036f3153a24",
      "59275cea094a4377838c8d74ff3253fe",
      "e552687f48304d0292ae35a0538988cc",
      "07e88d3883bf425c84bcfcf4caa457da",
      "1eff706f113843eb85835247dde2d552",
      "72945591d4cb42f6a035281da8111fef",
      "6cdc4f9acd20440589d72d94da1a31fe",
      "95bdd3ef41c343e69ae322683bfea7ed",
      "1c66a5a8d6764011afd867a03b1abe94",
      "a0753eeb3c5844ebaee0451c482b0358",
      "f77bd25a52c74b9ea635f18cba79cf72",
      "a88fc427f0f04aafae24782effc913d0",
      "4ae9fcde014d42558be2900404733b1c",
      "e4ef1500979347f9900b0e5703be89dd",
      "da4981885e08473681cec654ca107707",
      "1fc5becccb064c448ab0ceb270770689",
      "ad507f4d69c54e1e99702925524bbcf6",
      "f77088c95860486db1ad557a7c28f040",
      "3e4da5fa40f5469aa6581fbc30c20089",
      "391797bfd7c64dad854224aab8d71425",
      "55fd5260639b4767b1ffe0f4782b06d4",
      "263b230522c148e68eabf1a1b6119327",
      "5541a4effa9c4a35b57f295151f70374",
      "5b71c00d1b3b4e27997cc5a04121f40b",
      "ad59d454284641239187a2b1e68ef52b",
      "6b4eb9c8bfa948b9bba1ed591713ca9b",
      "eec30917266f473aa32cd22aaee2834b",
      "fed11d2e57014c448cb0b3e69d8c5db7",
      "b0fd91c116bd43b6b88d760eaae1e63c",
      "be38f17973fd4c4abdecb0a16ae4ce8a",
      "ddf8bcfd54534e89a3b1d15522f88b2b",
      "53aeb71171a949039d85ac93f23f326c",
      "80308d0615a142acb031fa8d20fcdc22",
      "9ab081ddad4947f697bc9568c291b3e4",
      "1809dce3903b41e6a558486126a17409",
      "0512a7c631ec4576a68cd519dbd6dfad",
      "990353049a5a40998fb6756dc0313b66",
      "ec6e17ce2d93429785bacfa44a813ad7",
      "edec3ca65d8745b0885745ca4bb9fd84",
      "8a8622f5760c457ea6d64515570e8681",
      "2dfaa50b0dde42de8836476773691def",
      "3cfdbe6e7e554d40b61d18cbdae7fee6",
      "66da6d8d9fb7493a90cfb38ff5884664",
      "c465dd16751e48c1a2b49dbcdbe58d7d",
      "11f3f5f7d40649a59dfb6ef619d306b5",
      "375625ed0d3e4e2f9b033d39d69cdd41",
      "b42c0d375b904b15af05652ac41f1d35",
      "d3ad5f095a6a4344818ab22740660c54",
      "a90abc5180444cadb435339a3aae7f1c",
      "274615e20dc445e78c85b263f308bf3f",
      "b82dbd928bfb4841a6b5bd636a251228",
      "de405c4617984e38895143fbb8aa63ca",
      "dc4b680a562a410db5eb9834d6bdc457",
      "f9d6fb7d938a45dd8b752be90bf9fb16",
      "66ae3ad6784f46a7925d2be1feb0c712",
      "f82b26cc5ff84074a74e6ffa00d68b9c",
      "47820020c35946839fd3e2a8cb1d971f",
      "b656ae4d84464f5b8179204444117e87",
      "f4ad2687f0d1453e9bdb7e6935af2b91",
      "0bea1fcbf0b2413cb3d1d8f042f0b9fe",
      "6bfb3d90df6740e1b5d841ddabecc313",
      "9f9c4922129a46fb81bb74fbb373ef45",
      "f913a72147bf435eb2a6adb7acefaeed",
      "9b54a212abd54fe6a1e44d6398c67347",
      "c78694aebe814b30a46021922545df0b",
      "aed8347672f74542a083d96f4eebc8bc",
      "d48c8da7acaf48ab801ad9eb3222b24b",
      "7f5b3a3bdf9143198d9410f4bef29413",
      "b3927d4cbb75486193da2883025c768f",
      "137f099fe16a4d17b34676e3f494ab1c",
      "3d940d0eff9d488d981772baa56081de",
      "e53434c6cccd474ea4837decefaa37b7",
      "d776d6a2b566403b92fef1408f414300",
      "8b0b59f6957b4ff3ab2283fc99fc161c",
      "a87c538ff57d4130809c0259a0b46265",
      "704428da7f2c458cbe1874d9db4dc8ef",
      "ac8914d7c3f445d8b3eb86533f053bdf",
      "0bdf0d21c5654f4597c1427c3549dcf2",
      "d12d780bccb44e41b04e47aa7e07f4b0",
      "18df48a9b8b74a329760208f18683d92",
      "ef3a33f2d6ae4733b4e704119e5757d8",
      "b655fb65bddc425a8942347f8e6288ab",
      "0add4d1173684dd69f29d5650983773a",
      "f8c04aff33624781831c04f57d06532d",
      "7a4376d6f3464a5ea723e3e54ecc32c4",
      "880fe11ede244bbe83b73110585f4dfd",
      "7e5391375335451aab920199d8495bba",
      "338c9d6a4c5745c3a101e0f017cc416d",
      "a3aecff3589249239290328df7380064",
      "96098fff21be48b9b33d12a1014de433",
      "8ce72702d5c04e6d85e0cb8e6ac3f15e",
      "37de1382383e4cd1b7527a07b3ce0cc6",
      "231fd2676e794d9f8d0b9f642c381eda",
      "e2172aa51bb845759550c4b134196e4c",
      "7c329b7c6e504a31bedbf631a147d66f",
      "d674e79ac91b4b5f87e10dfcf24a3ca6",
      "03266382c42d47daa4dfe6d74c6901a2",
      "dbc1d8a3ab04424a8d29b66a5849c744",
      "05bac33c8eff400eb343df057338271b",
      "69e77c2084d448779238e8b3a520b95e",
      "c0f51074e23248aabe9d771e952f7edb",
      "c53b78edf0d941219a98e7d7503f2008",
      "ed2ea6db289c4f28a6977b8420b58cb7",
      "5e60016b097c4eb88cb470a2a96e2bea",
      "d861984339584d7c8770f2d8e47a2e1f",
      "0b57382cf4a743db887f77e7ae2df335",
      "d40761f0fbbb4922b121a8d86a337a30",
      "4227b2cc956249a1820b8fd89c1d8d7f",
      "6ccb5e46113a49f6973b3851683bc68d",
      "4739c3694f8247e58339fe68fc670a72",
      "595f70423e104ebc8b551fb7c6381969",
      "a1f9bc7c51fc4e54bb955cc779d94383",
      "7bac68ff332c488291b8a240fd096795",
      "a4654915bda84c6eb42e88570f3707e2",
      "8eb3a5811bac41228090dcb565ba435b",
      "0d076fc7890a460989dd00b21ca2f1bc",
      "cc463782d73446b69466bb60401a62df",
      "1a0ca0d0e22744a8b2feae1027ffa460",
      "ca3b76a7923843219de9448027f48ef3",
      "26bd8efb173849b0a800c9681e3949f4",
      "372e03139a7c4c07b8b146935828bc7d",
      "d8a3cc72f9dc43768e2aa885f5866494",
      "41f3d4ae371f4ced8668d06688a39b0e",
      "7ea2762b3c2243bfa10a142d42136ef0",
      "09e9b4e769794166a8140f003ad22b85",
      "e4ee494660da4ce99470924ebc8c7178",
      "c197cf436a5a44b5b278eae5808ca4cc",
      "34d9929c83634623a05c49dc8862b0e2",
      "d1f41786b3c84406a0e4a88ef1867d31",
      "4621ff296c6247cbbd21494a5e787a68",
      "d4ab2dec05a847968d84cd2da6801154",
      "50fd7b62105e4aff9be07bdfad38b7da",
      "2158592e519d442093610bb6dc4beb38",
      "92472909bca149058a870c6146da068e",
      "78bcc39a3f854c268eb3ef1e028b1f10"
     ]
    },
    "id": "XpAQ8F0vR_LV",
    "outputId": "b835802b-98b8-4ed0-c5a1-7722d154c1d3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import requests\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Text Embedding and ML Models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The classifiers we will compare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping\n",
    "\n",
    "# imblearn is no longer needed for undersampling as we are doing it manually\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# --- Configuration for file paths ---\n",
    "# Base directories\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "VERSION = \"4.0\"\n",
    "\n",
    "\n",
    "# Subdirectories for data\n",
    "NVD_DATA_DIR = DATA_DIR / VERSION / \"nvd_data\"\n",
    "GARAK_DATA_DIR = DATA_DIR / VERSION / \"garak\"\n",
    "\n",
    "# Specific file paths\n",
    "PARSED_DATA_PATH = NVD_DATA_DIR / f\"all_nvd_cves.pkl\"\n",
    "GARAK_REPORT_JSONL = GARAK_DATA_DIR / \"garak.report.jsonl\"\n",
    "GARAK_REPORT_CSV = GARAK_DATA_DIR / \"garak_report_flat.csv\"\n",
    "\n",
    "# Model file paths\n",
    "MODEL_PATH = MODELS_DIR /VERSION/ \"best_cvss_classifier_historic.pkl\"\n",
    "LABEL_ENCODER_PATH = MODELS_DIR /VERSION /\"cvss_label_encoder_historic.pkl\"\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1A: Process Garak Report\n",
    "# ----------------------------------------\n",
    "def process_garak_report():\n",
    "    \"\"\"\n",
    "    Downloads a sample Garak report if not present, and converts it\n",
    "    from .jsonl format to a flattened .csv file.\n",
    "    \"\"\"\n",
    "    GARAK_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url = \"https://gist.githubusercontent.com/shubhobm/9fa52d71c8bb36bfb888eee2ba3d18f2/raw/ef1808e6d3b26002d9b046e6c120d438adf49008/gpt35-0906.report.jsonl\"\n",
    "    if not GARAK_REPORT_JSONL.exists():\n",
    "        print(\"Downloading sample Garak report...\")\n",
    "        urllib.request.urlretrieve(url, GARAK_REPORT_JSONL)\n",
    "        print(f\"✅ Downloaded: {GARAK_REPORT_JSONL}\")\n",
    "\n",
    "    def parse_status(status_code):\n",
    "        return {1: \"Pass\", 2: \"Fail\"}.get(status_code, \"Not Evaluated\")\n",
    "\n",
    "    def extract_input_output(record):\n",
    "        turns = record.get(\"notes\", {}).get(\"turns\", [])\n",
    "        if turns:\n",
    "            attacker = \" | \".join([msg.strip().replace(\"\\n\", \" \") for role, msg in turns if role == \"probe\"])\n",
    "            bot = \" | \".join([msg.strip().replace(\"\\n\", \" \") for role, msg in turns if role == \"model\"])\n",
    "            return attacker, bot\n",
    "        prompt = record.get(\"prompt\", \"\").strip().replace(\"\\n\", \" \")\n",
    "        outputs = \" | \".join([o.strip().replace(\"\\n\", \" \") for o in record.get(\"outputs\", [])])\n",
    "        return prompt, outputs\n",
    "\n",
    "    with open(GARAK_REPORT_JSONL, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(GARAK_REPORT_CSV, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "\n",
    "        fieldnames = [\"uuid\", \"probe_classname\", \"attacker_input\", \"target_bot_response\", \"status\", \"goal\", \"trigger\"]\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            if record.get(\"entry_type\") != \"attempt\":\n",
    "                continue\n",
    "\n",
    "            attacker_input, bot_response = extract_input_output(record)\n",
    "            writer.writerow({\n",
    "                \"uuid\": record.get(\"uuid\", \"\"),\n",
    "                \"probe_classname\": record.get(\"probe_classname\", \"\"),\n",
    "                \"attacker_input\": attacker_input,\n",
    "                \"target_bot_response\": bot_response,\n",
    "                \"status\": parse_status(record.get(\"status\")),\n",
    "                \"goal\": record.get(\"goal\", \"\"),\n",
    "                \"trigger\": record.get(\"notes\", {}).get(\"trigger\", \"\")\n",
    "            })\n",
    "    print(f\"✅ Garak report successfully converted to: {GARAK_REPORT_CSV}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1B: Download and Parse All Historical NVD Data\n",
    "# ----------------------------------------\n",
    "def download_and_parse_all_nvd_data():\n",
    "    \"\"\"\n",
    "    Downloads all NVD CVE data, parses them, removes duplicates, and saves\n",
    "    the result to a pickle file.\n",
    "    \"\"\"\n",
    "    NVD_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BASE_URL = \"https://nvd.nist.gov/feeds/json/cve/1.1/\"\n",
    "    START_YEAR, CURRENT_YEAR = 2002, datetime.now().year\n",
    "\n",
    "    print(\"--- Starting NVD Data Download ---\")\n",
    "    for year in range(START_YEAR, CURRENT_YEAR + 1):\n",
    "        filename = f\"nvdcve-1.1-{year}.json.gz\"\n",
    "        download_path = NVD_DATA_DIR / filename\n",
    "        if download_path.exists(): continue\n",
    "        print(f\"Downloading: {filename}\")\n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{filename}\", stream=True, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                with open(download_path, 'wb') as f: f.writelines(response.iter_content(8192))\n",
    "            else: print(f\" -> Failed: HTTP {response.status_code}\")\n",
    "        except requests.RequestException as e: print(f\" -> Error: {e}\")\n",
    "\n",
    "    print(\"\\n--- Starting NVD Data Parsing ---\")\n",
    "    parsed_cve_list = []\n",
    "    for file_path in sorted(NVD_DATA_DIR.glob('*.json.gz')):\n",
    "        print(f\"Parsing {file_path.name}...\")\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            cve_data = json.load(f)\n",
    "        for item in cve_data.get(\"CVE_Items\", []):\n",
    "            description = next((d[\"value\"] for d in item.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", []) if d.get(\"lang\") == \"en\"), \"\")\n",
    "            impact = item.get(\"impact\", {})\n",
    "            severity = impact.get('baseMetricV3', {}).get('cvssV3', {}).get('baseSeverity') or impact.get('baseMetricV2', {}).get('severity')\n",
    "            # MODIFIED: Also extract the published date to identify recent CVEs\n",
    "            published_date = item.get('publishedDate')\n",
    "            if description and severity and published_date:\n",
    "                parsed_cve_list.append({\n",
    "                    \"description\": description.strip(),\n",
    "                    \"severity\": severity.strip().capitalize(),\n",
    "                    \"publishedDate\": published_date\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(parsed_cve_list)\n",
    "    print(f\"\\nEntries before duplicate removal: {len(df)}\")\n",
    "    df.drop_duplicates(subset=['description'], keep='last', inplace=True)\n",
    "    print(f\"Entries after duplicate removal: {len(df)}\")\n",
    "    df.to_pickle(PARSED_DATA_PATH)\n",
    "    print(f\"✅ Parsing Complete. Saved {len(df)} unique entries to {PARSED_DATA_PATH}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 2: Find the Best Classifier and Train It\n",
    "# ----------------------------------------\n",
    "def train_and_evaluate_models():\n",
    "    \"\"\"\n",
    "    Loads data, creates a balanced dataset by undersampling the majority classes\n",
    "    (keeping the most recent data), then splits this balanced set for training\n",
    "    and evaluation.\n",
    "    \"\"\"\n",
    "    if not PARSED_DATA_PATH.exists():\n",
    "        print(f\"Error: Parsed data not found at {PARSED_DATA_PATH}. Please run the 'data_prep' workflow first.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"\\n--- Training and Evaluation with Undersampling ---\")\n",
    "    df = pd.read_pickle(PARSED_DATA_PATH).dropna()\n",
    "    print(f\"Loaded {len(df)} valid NVD entries.\")\n",
    "\n",
    "    # --- CORRECTED METHOD: Undersample BEFORE splitting ---\n",
    "    # 1. Convert 'publishedDate' to datetime objects to allow sorting\n",
    "    df['publishedDate'] = pd.to_datetime(df['publishedDate'])\n",
    "\n",
    "    # 2. Sort by date so newest entries are first\n",
    "    df = df.sort_values('publishedDate', ascending=False)\n",
    "\n",
    "    # 3. Determine the size of the smallest class\n",
    "    n_samples = df['severity'].value_counts().min()\n",
    "    print(f\"\\nSmallest class size is {n_samples}. Undersampling all classes to this size.\")\n",
    "    print(\"Original dataset distribution:\\n\", df['severity'].value_counts())\n",
    "\n",
    "    # 4. Group by severity and take the `n_samples` most recent entries from each group\n",
    "    df_balanced = df.groupby('severity').head(n_samples)\n",
    "\n",
    "    # 5. Shuffle the balanced dataset to ensure randomness\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nBalanced dataset distribution:\\n\", df_balanced['severity'].value_counts())\n",
    "\n",
    "    # 6. Prepare the final text list and labels from the balanced dataframe\n",
    "    X_text = df_balanced['description'].tolist()\n",
    "    y_labels = df_balanced['severity'].tolist()\n",
    "\n",
    "    # --- End of Undersampling Modification ---\n",
    "\n",
    "    print(\"\\nLoading embedding model: 'all-MiniLM-L6-v2'...\")\n",
    "    embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    print(\"Encoding all descriptions from the balanced dataset...\")\n",
    "    X_embeddings = embed_model.encode(X_text, show_progress_bar=True)\n",
    "\n",
    "    # Encode the labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_labels)\n",
    "\n",
    "    # 7. Now split the balanced and encoded data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_embeddings, y_encoded, test_size=0.25, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    print(f\"\\nTraining on {len(X_train)} samples, testing on {len(X_test)} samples.\")\n",
    "\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(n_jobs=-1, random_state=42, n_estimators=100),\n",
    "        \"LightGBM (Tuned)\": lgb.LGBMClassifier(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            colsample_bytree=0.8,\n",
    "            subsample=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1\n",
    "        )\n",
    "    }\n",
    "\n",
    "    best_f1, best_model_name, best_classifier_obj = -1, \"\", None\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        fit_params = {}\n",
    "        if \"LightGBM\" in name:\n",
    "            # The validation set for early stopping should come from the split data\n",
    "            fit_params = {\"eval_set\": [(X_test, y_test)], \"callbacks\": [early_stopping(20, verbose=False)]}\n",
    "\n",
    "        clf.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        f1_score = report[\"weighted avg\"][\"f1-score\"]\n",
    "        if f1_score > best_f1:\n",
    "            best_f1, best_model_name, best_classifier_obj = f1_score, name, clf\n",
    "        print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "    print(f\"\\n🏆 Best performing model is: {best_model_name} with F1-Score: {best_f1:.4f}\")\n",
    "\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(best_classifier_obj, MODEL_PATH)\n",
    "    joblib.dump(le, LABEL_ENCODER_PATH)\n",
    "    print(f\"✅ Best model saved to {MODEL_PATH}\")\n",
    "\n",
    "    return embed_model, best_classifier_obj, le\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 3: Predict using a saved model\n",
    "# ----------------------------------------\n",
    "def predict_on_garak(embed_model=None, classifier=None, label_encoder=None):\n",
    "    if not GARAK_REPORT_CSV.exists():\n",
    "        print(f\"Error: Garak CSV not found. Please run the 'data_prep' workflow first.\")\n",
    "        return\n",
    "\n",
    "    if not all([embed_model, classifier, label_encoder]):\n",
    "        print(f\"Loading models from disk...\")\n",
    "        if not MODEL_PATH.exists():\n",
    "            print(f\"Error: Model file not found at {MODEL_PATH}. Please run the 'train' workflow first.\")\n",
    "            return\n",
    "        classifier = joblib.load(MODEL_PATH)\n",
    "        label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "        # Use the same model for consistency\n",
    "        embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    print(\"\\n--- Prediction on Garak Report ---\")\n",
    "    df = pd.read_csv(GARAK_REPORT_CSV)\n",
    "    df[\"full_text\"] = df[\"target_bot_response\"].fillna('')\n",
    "\n",
    "    print(\"Embedding Garak report for prediction...\")\n",
    "    embeddings = embed_model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "    probabilities = classifier.predict_proba(embeddings)\n",
    "    df[\"predicted_severity\"] = label_encoder.inverse_transform(np.argmax(probabilities, axis=1))\n",
    "    df[\"confidence_score\"] = np.round(np.max(probabilities, axis=1), 4)\n",
    "    for i, name in enumerate(label_encoder.classes_):\n",
    "        df[f'prob_{name.lower()}'] = np.round(probabilities[:, i], 4)\n",
    "\n",
    "    output_path = GARAK_DATA_DIR / \"garak_with_severity_historic.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Predictions saved to {output_path}\")\n",
    "\n",
    "    print(\"\\n--- Final Vulnerability Score (for FAILED test cases) ---\")\n",
    "    failed_df = df[df['status'] == 'Fail'].copy()\n",
    "\n",
    "    if failed_df.empty:\n",
    "        print(\"No failed test cases found in the report. No score to calculate.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Calculating score based on {len(failed_df)} failed test cases (out of {len(df)} total).\")\n",
    "    severity_map = {'Critical': 10, 'High': 7, 'Medium': 4, 'Low': 1}\n",
    "    severity_counts = failed_df['predicted_severity'].value_counts()\n",
    "    total_score = sum(count * severity_map.get(s, 0) for s, count in severity_counts.items())\n",
    "    max_possible_score = len(failed_df) * 10\n",
    "    normalized_score = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0\n",
    "\n",
    "    print(\"\\nSeverity Distribution (of Failures):\")\n",
    "    print(severity_counts)\n",
    "    print(f\"\\nTotal Raw Risk Score (from Failures): {total_score}\")\n",
    "    print(f\"Normalized Report Vulnerability Score (0-100): {normalized_score:.2f}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 4: Create a ZIP Archive of the Results\n",
    "# ----------------------------------------\n",
    "def create_archive():\n",
    "    print(\"\\n--- Creating ZIP Archive ---\")\n",
    "    archive_name = \"cve_prediction_archive\"\n",
    "\n",
    "    with zipfile.ZipFile(f\"{archive_name}.zip\", 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for folder in [DATA_DIR, MODELS_DIR]:\n",
    "            if folder.exists() and folder.is_dir():\n",
    "                for file_path in folder.rglob('*'):\n",
    "                    zipf.write(file_path, arcname=file_path.relative_to(Path.cwd()))\n",
    "                print(f\"Archived folder: {folder}\")\n",
    "            else:\n",
    "                print(f\"Warning: Folder '{folder}' not found. Skipping.\")\n",
    "\n",
    "    print(f\"✅ Archive created successfully: {archive_name}.zip\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# MAIN EXECUTION WORKFLOW\n",
    "# ----------------------------------------\n",
    "def run_workflow(data_prep=False, train=False, predict=False, archive=False):\n",
    "    \"\"\"\n",
    "    Controls the main execution flow of the script.\n",
    "    Set flags to True for the steps you want to run.\n",
    "    \"\"\"\n",
    "    print(\"--- CVE Severity Prediction Workflow ---\")\n",
    "\n",
    "    if data_prep:\n",
    "        print(\"\\n=== STAGE 1: DATA PREPARATION ===\")\n",
    "        process_garak_report()\n",
    "        download_and_parse_all_nvd_data()\n",
    "\n",
    "    embed_model, classifier, label_encoder = None, None, None\n",
    "    if train:\n",
    "        print(\"\\n=== STAGE 2: MODEL TRAINING ===\")\n",
    "        embed_model, classifier, label_encoder = train_and_evaluate_models()\n",
    "\n",
    "    if predict:\n",
    "        print(\"\\n=== STAGE 3: PREDICTION ===\")\n",
    "        predict_on_garak(embed_model, classifier, label_encoder)\n",
    "\n",
    "    if archive:\n",
    "        print(\"\\n=== STAGE 4: ARCHIVING ===\")\n",
    "        create_archive()\n",
    "\n",
    "    print(\"\\n--- Workflow Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configure your desired workflow here ---\n",
    "\n",
    "    # To get good results, you MUST run data_prep=True first.\n",
    "    # On the first run, it's recommended to do all three main steps.\n",
    "    run_workflow(data_prep=True, train=True, predict=True)\n",
    "\n",
    "    # After the first run, you can comment out the line above and\n",
    "    # uncomment one of the following lines to run specific tasks.\n",
    "\n",
    "    # Example: Just run prediction using existing models\n",
    "    # run_workflow(predict=True)\n",
    "\n",
    "    # Example: Just create an archive of existing results\n",
    "    # run_workflow(archive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4p9DZZ4VgSgl",
    "outputId": "30444dbc-e109-46bc-b31f-867d0c243e86"
   },
   "outputs": [],
   "source": [
    "!zip -r data.zip data/\n",
    "!zip -r models.zip models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1cAatF2ZIXa"
   },
   "outputs": [],
   "source": [
    "!rm -rf data/\n",
    "!rm -rf models/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
