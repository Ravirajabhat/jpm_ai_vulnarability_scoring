{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufEda95T_0hM"
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg5rctvg-7DM"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install sentence-transformers torch pandas scikit-learn joblib requests lightgbm torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "00a25ca19b9346039c8b16f88436a70e",
      "ff5e34c18fec431f938323264b2a340d",
      "2d66f19261d44138a7870fcc5747a48f",
      "39a65500b14a4e459900e276f3c778f5",
      "4b65f7dd86ef44b6ac8fb6f71bd0ab97",
      "1941133844924c27a09d5e9df661ae5a",
      "96836d06c9854f0bbe32c8396fbc4b3e",
      "e4a86c010e6b423a8b3f17396f5e9ae5",
      "2dde478f0b8542d997d65dee7ee3e846",
      "50de6b079598463a82aa0c066a154260",
      "90b67479e07848bb9634ee743be16717",
      "0a55ce5a584647c0bd2c90efe7e77514",
      "e00ec7bdc30a4851aabc0382888c9fbb",
      "b29e23c762024ee9aacbb2b0290433d4",
      "3cd7fc18801b415d92749f3511212381",
      "6d855584486e4eebbabde44f6e23a0de",
      "a6e734482fb443c5b9fd1846a73ff6c7",
      "9d58f81737e948b6bf950c0ebbbf666e",
      "f1ed725c3c334956a097ddfc493296d4",
      "667428fa87024dab87a3fc56dda11ee0",
      "dc81b70be95045a187bc4ed39280c34c",
      "07a583fdd1154657b9ea02728d0bdc3d"
     ]
    },
    "id": "hpG6Ruv7--wH",
    "outputId": "9f4b8e26-62bc-4387-b469-b8181ae8c7d1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import requests\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Text Embedding and ML Models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The classifiers we will compare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping\n",
    "\n",
    "# Import SMOTE for handling class imbalance\n",
    "# You may need to install this: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# --- Configuration for file paths ---\n",
    "# Base directories\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "VERSION = \"3.0\"\n",
    "\n",
    "\n",
    "# Subdirectories for data\n",
    "NVD_DATA_DIR = DATA_DIR / VERSION / \"nvd_data\"\n",
    "GARAK_DATA_DIR = DATA_DIR / VERSION / \"garak\"\n",
    "\n",
    "# Specific file paths\n",
    "PARSED_DATA_PATH = NVD_DATA_DIR / f\"all_nvd_cves.pkl\"\n",
    "GARAK_REPORT_JSONL = GARAK_DATA_DIR / \"garak.report.jsonl\"\n",
    "GARAK_REPORT_CSV = GARAK_DATA_DIR / \"garak_report_flat.csv\"\n",
    "\n",
    "# Model file paths\n",
    "MODEL_PATH = MODELS_DIR /VERSION/ \"best_cvss_classifier_historic.pkl\"\n",
    "LABEL_ENCODER_PATH = MODELS_DIR /VERSION /\"cvss_label_encoder_historic.pkl\"\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1A: Process Garak Report\n",
    "# ----------------------------------------\n",
    "def process_garak_report():\n",
    "    \"\"\"\n",
    "    Downloads a sample Garak report if not present, and converts it\n",
    "    from .jsonl format to a flattened .csv file.\n",
    "    \"\"\"\n",
    "    GARAK_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url = \"https://gist.githubusercontent.com/shubhobm/9fa52d71c8bb36bfb888eee2ba3d18f2/raw/ef1808e6d3b26002d9b046e6c120d438adf49008/gpt35-0906.report.jsonl\"\n",
    "    if not GARAK_REPORT_JSONL.exists():\n",
    "        print(\"Downloading sample Garak report...\")\n",
    "        urllib.request.urlretrieve(url, GARAK_REPORT_JSONL)\n",
    "        print(f\"âœ… Downloaded: {GARAK_REPORT_JSONL}\")\n",
    "\n",
    "    def parse_status(status_code):\n",
    "        return {1: \"Pass\", 2: \"Fail\"}.get(status_code, \"Not Evaluated\")\n",
    "\n",
    "    def extract_input_output(record):\n",
    "        turns = record.get(\"notes\", {}).get(\"turns\", [])\n",
    "        if turns:\n",
    "            attacker = \" | \".join([msg.strip().replace(\"\\n\", \" \") for role, msg in turns if role == \"probe\"])\n",
    "            bot = \" | \".join([msg.strip().replace(\"\\n\", \" \") for role, msg in turns if role == \"model\"])\n",
    "            return attacker, bot\n",
    "        prompt = record.get(\"prompt\", \"\").strip().replace(\"\\n\", \" \")\n",
    "        outputs = \" | \".join([o.strip().replace(\"\\n\", \" \") for o in record.get(\"outputs\", [])])\n",
    "        return prompt, outputs\n",
    "\n",
    "    with open(GARAK_REPORT_JSONL, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(GARAK_REPORT_CSV, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "\n",
    "        fieldnames = [\"uuid\", \"probe_classname\", \"attacker_input\", \"target_bot_response\", \"status\", \"goal\", \"trigger\"]\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            if record.get(\"entry_type\") != \"attempt\":\n",
    "                continue\n",
    "\n",
    "            attacker_input, bot_response = extract_input_output(record)\n",
    "            writer.writerow({\n",
    "                \"uuid\": record.get(\"uuid\", \"\"),\n",
    "                \"probe_classname\": record.get(\"probe_classname\", \"\"),\n",
    "                \"attacker_input\": attacker_input,\n",
    "                \"target_bot_response\": bot_response,\n",
    "                \"status\": parse_status(record.get(\"status\")),\n",
    "                \"goal\": record.get(\"goal\", \"\"),\n",
    "                \"trigger\": record.get(\"notes\", {}).get(\"trigger\", \"\")\n",
    "            })\n",
    "    print(f\"âœ… Garak report successfully converted to: {GARAK_REPORT_CSV}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1B: Download and Parse All Historical NVD Data\n",
    "# ----------------------------------------\n",
    "def download_and_parse_all_nvd_data():\n",
    "    \"\"\"\n",
    "    Downloads all NVD CVE data, parses them, removes duplicates, and saves\n",
    "    the result to a pickle file.\n",
    "    \"\"\"\n",
    "    NVD_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BASE_URL = \"https://nvd.nist.gov/feeds/json/cve/1.1/\"\n",
    "    START_YEAR, CURRENT_YEAR = 2002, datetime.now().year\n",
    "\n",
    "    print(\"--- Starting NVD Data Download ---\")\n",
    "    for year in range(START_YEAR, CURRENT_YEAR + 1):\n",
    "        filename = f\"nvdcve-1.1-{year}.json.gz\"\n",
    "        download_path = NVD_DATA_DIR / filename\n",
    "        if download_path.exists(): continue\n",
    "        print(f\"Downloading: {filename}\")\n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{filename}\", stream=True, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                with open(download_path, 'wb') as f: f.writelines(response.iter_content(8192))\n",
    "            else: print(f\" -> Failed: HTTP {response.status_code}\")\n",
    "        except requests.RequestException as e: print(f\" -> Error: {e}\")\n",
    "\n",
    "    print(\"\\n--- Starting NVD Data Parsing ---\")\n",
    "    parsed_cve_list = []\n",
    "    for file_path in sorted(NVD_DATA_DIR.glob('*.json.gz')):\n",
    "        print(f\"Parsing {file_path.name}...\")\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            cve_data = json.load(f)\n",
    "        for item in cve_data.get(\"CVE_Items\", []):\n",
    "            description = next((d[\"value\"] for d in item.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", []) if d.get(\"lang\") == \"en\"), \"\")\n",
    "            impact = item.get(\"impact\", {})\n",
    "            severity = impact.get('baseMetricV3', {}).get('cvssV3', {}).get('baseSeverity') or impact.get('baseMetricV2', {}).get('severity')\n",
    "            if description and severity:\n",
    "                parsed_cve_list.append({\"description\": description.strip(), \"severity\": severity.strip().capitalize()})\n",
    "\n",
    "    df = pd.DataFrame(parsed_cve_list)\n",
    "    print(f\"\\nEntries before duplicate removal: {len(df)}\")\n",
    "    df.drop_duplicates(subset=['description'], keep='last', inplace=True)\n",
    "    print(f\"Entries after duplicate removal: {len(df)}\")\n",
    "    df.to_pickle(PARSED_DATA_PATH)\n",
    "    print(f\"âœ… Parsing Complete. Saved {len(df)} unique entries to {PARSED_DATA_PATH}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 2: Find the Best Classifier and Train It\n",
    "# ----------------------------------------\n",
    "def train_and_evaluate_models():\n",
    "    \"\"\"\n",
    "    Loads data, trains classifiers on a balanced dataset, finds the best one,\n",
    "    and saves it along with the label encoder.\n",
    "    \"\"\"\n",
    "    if not PARSED_DATA_PATH.exists():\n",
    "        print(f\"Error: Parsed data not found at {PARSED_DATA_PATH}. Please run the 'data_prep' workflow first.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"\\n--- Training and Evaluation ---\")\n",
    "    df = pd.read_pickle(PARSED_DATA_PATH).dropna()\n",
    "    print(f\"Training on {len(df)} valid NVD entries. This is the correct number if data prep was successful.\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['severity'])\n",
    "\n",
    "    print(\"Loading embedding model: 'all-MiniLM-L6-v2'...\")\n",
    "    embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    print(\"\\nEncoding all descriptions... (This may take a very long time)\")\n",
    "    X = embed_model.encode(df['description'].tolist(), show_progress_bar=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "    print(\"\\nApplying SMOTE to balance the training data...\")\n",
    "    # CORRECTED: The 'n_jobs' parameter was removed as it's no longer supported.\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(n_jobs=-1, random_state=42, n_estimators=100),\n",
    "        \"LightGBM (Tuned)\": lgb.LGBMClassifier(\n",
    "            n_estimators=1000,      # Increase max trees\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            colsample_bytree=0.8,    # Feature subsampling\n",
    "            subsample=0.8,           # Row subsampling\n",
    "            reg_alpha=0.1,           # L1 regularization\n",
    "            reg_lambda=0.1           # L2 regularization\n",
    "        )\n",
    "    }\n",
    "\n",
    "    best_f1, best_model_name, best_classifier_obj = -1, \"\", None\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        fit_params = {}\n",
    "        if \"LightGBM\" in name:\n",
    "            # Use early stopping to prevent overfitting and find the optimal number of trees\n",
    "            fit_params = {\"eval_set\": [(X_test, y_test)], \"callbacks\": [early_stopping(20, verbose=False)]}\n",
    "\n",
    "        clf.fit(X_train_resampled, y_train_resampled, **fit_params)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        f1_score = report[\"weighted avg\"][\"f1-score\"]\n",
    "        if f1_score > best_f1:\n",
    "            best_f1, best_model_name, best_classifier_obj = f1_score, name, clf\n",
    "        print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "    print(f\"\\nðŸ† Best performing model is: {best_model_name} with F1-Score: {best_f1:.4f}\")\n",
    "\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(best_classifier_obj, MODEL_PATH)\n",
    "    joblib.dump(le, LABEL_ENCODER_PATH)\n",
    "    print(f\"âœ… Best model saved to {MODEL_PATH}\")\n",
    "\n",
    "    return embed_model, best_classifier_obj, le\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 3: Predict using a saved model\n",
    "# ----------------------------------------\n",
    "def predict_on_garak(embed_model=None, classifier=None, label_encoder=None):\n",
    "    if not GARAK_REPORT_CSV.exists():\n",
    "        print(f\"Error: Garak CSV not found. Please run the 'data_prep' workflow first.\")\n",
    "        return\n",
    "\n",
    "    if not all([embed_model, classifier, label_encoder]):\n",
    "        print(f\"Loading models from disk...\")\n",
    "        if not MODEL_PATH.exists():\n",
    "            print(f\"Error: Model file not found at {MODEL_PATH}. Please run the 'train' workflow first.\")\n",
    "            return\n",
    "        classifier = joblib.load(MODEL_PATH)\n",
    "        label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "        embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    print(\"\\n--- Prediction on Garak Report ---\")\n",
    "    df = pd.read_csv(GARAK_REPORT_CSV)\n",
    "    df[\"full_text\"] = df[\"attacker_input\"].fillna('') + \" \" + df[\"target_bot_response\"].fillna('')\n",
    "\n",
    "    print(\"Embedding Garak report for prediction...\")\n",
    "    embeddings = embed_model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "    probabilities = classifier.predict_proba(embeddings)\n",
    "    df[\"predicted_severity\"] = label_encoder.inverse_transform(np.argmax(probabilities, axis=1))\n",
    "    df[\"confidence_score\"] = np.round(np.max(probabilities, axis=1), 4)\n",
    "    for i, name in enumerate(label_encoder.classes_):\n",
    "        df[f'prob_{name.lower()}'] = np.round(probabilities[:, i], 4)\n",
    "\n",
    "    output_path = GARAK_DATA_DIR / \"garak_with_severity_historic.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to {output_path}\")\n",
    "\n",
    "    print(\"\\n--- Final Vulnerability Score (for FAILED test cases) ---\")\n",
    "    failed_df = df[df['status'] == 'Fail'].copy()\n",
    "\n",
    "    if failed_df.empty:\n",
    "        print(\"No failed test cases found in the report. No score to calculate.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Calculating score based on {len(failed_df)} failed test cases (out of {len(df)} total).\")\n",
    "    severity_map = {'Critical': 10, 'High': 7, 'Medium': 4, 'Low': 1}\n",
    "    severity_counts = failed_df['predicted_severity'].value_counts()\n",
    "    total_score = sum(count * severity_map.get(s, 0) for s, count in severity_counts.items())\n",
    "    max_possible_score = len(failed_df) * 10\n",
    "    normalized_score = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0\n",
    "\n",
    "    print(\"\\nSeverity Distribution (of Failures):\")\n",
    "    print(severity_counts)\n",
    "    print(f\"\\nTotal Raw Risk Score (from Failures): {total_score}\")\n",
    "    print(f\"Normalized Report Vulnerability Score (0-100): {normalized_score:.2f}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 4: Create a ZIP Archive of the Results\n",
    "# ----------------------------------------\n",
    "def create_archive():\n",
    "    print(\"\\n--- Creating ZIP Archive ---\")\n",
    "    archive_name = \"cve_prediction_archive\"\n",
    "\n",
    "    with zipfile.ZipFile(f\"{archive_name}.zip\", 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for folder in [DATA_DIR, MODELS_DIR]:\n",
    "            if folder.exists() and folder.is_dir():\n",
    "                for file_path in folder.rglob('*'):\n",
    "                    zipf.write(file_path, arcname=file_path.relative_to(Path.cwd()))\n",
    "                print(f\"Archived folder: {folder}\")\n",
    "            else:\n",
    "                print(f\"Warning: Folder '{folder}' not found. Skipping.\")\n",
    "\n",
    "    print(f\"âœ… Archive created successfully: {archive_name}.zip\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# MAIN EXECUTION WORKFLOW\n",
    "# ----------------------------------------\n",
    "def run_workflow(data_prep=False, train=False, predict=False, archive=False):\n",
    "    \"\"\"\n",
    "    Controls the main execution flow of the script.\n",
    "    Set flags to True for the steps you want to run.\n",
    "    \"\"\"\n",
    "    print(\"--- CVE Severity Prediction Workflow ---\")\n",
    "\n",
    "    if data_prep:\n",
    "        print(\"\\n=== STAGE 1: DATA PREPARATION ===\")\n",
    "        process_garak_report()\n",
    "        download_and_parse_all_nvd_data()\n",
    "\n",
    "    embed_model, classifier, label_encoder = None, None, None\n",
    "    if train:\n",
    "        print(\"\\n=== STAGE 2: MODEL TRAINING ===\")\n",
    "        embed_model, classifier, label_encoder = train_and_evaluate_models()\n",
    "\n",
    "    if predict:\n",
    "        print(\"\\n=== STAGE 3: PREDICTION ===\")\n",
    "        predict_on_garak(embed_model, classifier, label_encoder)\n",
    "\n",
    "    if archive:\n",
    "        print(\"\\n=== STAGE 4: ARCHIVING ===\")\n",
    "        create_archive()\n",
    "\n",
    "    print(\"\\n--- Workflow Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configure your desired workflow here ---\n",
    "\n",
    "    # To get good results, you MUST run data_prep=True first.\n",
    "    # On the first run, it's recommended to do all three main steps.\n",
    "    run_workflow(data_prep=True, train=True, predict=True)\n",
    "\n",
    "    # After the first run, you can comment out the line above and\n",
    "    # uncomment one of the following lines to run specific tasks.\n",
    "\n",
    "    # Example: Just run prediction using existing models\n",
    "    # run_workflow(predict=True)\n",
    "\n",
    "    # Example: Just create an archive of existing results\n",
    "    # run_workflow(archive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B0YTJd1TdTl",
    "outputId": "3fe2dfb3-c638-494b-a800-b1af5f3838f3"
   },
   "outputs": [],
   "source": [
    "!zip -r data.zip data/\n",
    "!zip -r models.zip models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0FqC4-cEY9C"
   },
   "outputs": [],
   "source": [
    "!rm -rf data/\n",
    "!rm -rf models/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
