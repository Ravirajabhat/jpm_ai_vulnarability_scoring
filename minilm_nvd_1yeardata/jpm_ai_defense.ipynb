{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c965780b",
   "metadata": {},
   "source": [
    "## Download Sample garak report for gpt35-0906.report.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600ab2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (4.6.0)\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.13.0 imblearn-0.0 sklearn-compat-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers torch pandas scikit-learn joblib requests lightgbm imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d0e36",
   "metadata": {},
   "source": [
    "## Download Sample garak report for gpt35-0906.report.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c289ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sample Garak report...\n",
      "âœ… Downloaded: garak.report.jsonl\n",
      "âœ… Garak report successfully converted to: garak_report_flat.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Download sample Garak report if not present\n",
    "url = \"https://gist.githubusercontent.com/shubhobm/9fa52d71c8bb36bfb888eee2ba3d18f2/raw/ef1808e6d3b26002d9b046e6c120d438adf49008/gpt35-0906.report.jsonl\"\n",
    "input_file = \"garak.report.jsonl\"\n",
    "if not os.path.exists(input_file):\n",
    "    print(\"Downloading sample Garak report...\")\n",
    "    urllib.request.urlretrieve(url, input_file)\n",
    "    print(\"âœ… Downloaded:\", input_file)\n",
    "\n",
    "output_file = \"garak_report_flat.csv\"\n",
    "\n",
    "# Status decoding\n",
    "def parse_status(status_code):\n",
    "    if status_code == 1:\n",
    "        return \"Pass\"\n",
    "    elif status_code == 2:\n",
    "        return \"Fail\"\n",
    "    else:\n",
    "        return \"Not Evaluated\"\n",
    "\n",
    "# Parse turns-based or prompt-based format\n",
    "def extract_input_output(record):\n",
    "    turns = record.get(\"notes\", {}).get(\"turns\", [])\n",
    "    if turns:  # Multi-turn conversation\n",
    "        attacker, bot = [], []\n",
    "        for role, msg in turns:\n",
    "            msg = msg.strip().replace(\"\\n\", \" \")\n",
    "            if role == \"probe\":\n",
    "                attacker.append(msg)\n",
    "            elif role == \"model\":\n",
    "                bot.append(msg)\n",
    "        return \" | \".join(attacker), \" | \".join(bot)\n",
    "\n",
    "    # Fallback to single-turn prompt + outputs\n",
    "    prompt = record.get(\"prompt\", \"\").strip().replace(\"\\n\", \" \")\n",
    "    outputs = record.get(\"outputs\", [])\n",
    "    output_texts = [o.strip().replace(\"\\n\", \" \") for o in outputs]\n",
    "    return prompt, \" | \".join(output_texts)\n",
    "\n",
    "# Main conversion loop\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=[\n",
    "        \"uuid\", \"probe_classname\", \"attacker_input\", \"target_bot_response\", \"status\", \"goal\", \"trigger\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for line in infile:\n",
    "        record = json.loads(line)\n",
    "        if record.get(\"entry_type\") != \"attempt\":\n",
    "            continue\n",
    "\n",
    "        uuid = record.get(\"uuid\", \"\")\n",
    "        probe_class = record.get(\"probe_classname\", \"\")\n",
    "        status = parse_status(record.get(\"status\"))\n",
    "        attacker_input, bot_response = extract_input_output(record)\n",
    "        goal = record.get(\"goal\", \"\")\n",
    "        trigger = record.get(\"notes\", {}).get(\"trigger\", \"\")\n",
    "\n",
    "        writer.writerow({\n",
    "            \"uuid\": uuid,\n",
    "            \"probe_classname\": probe_class,\n",
    "            \"attacker_input\": attacker_input,\n",
    "            \"target_bot_response\": bot_response,\n",
    "            \"status\": status,\n",
    "            \"goal\": goal,\n",
    "            \"trigger\": trigger\n",
    "        })\n",
    "\n",
    "print(f\"âœ… Garak report successfully converted to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007417f5",
   "metadata": {},
   "source": [
    "## Download Latest cves data from NVD to train own CVSS seviarity score Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "999d019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CVE Severity Prediction Workflow ---\n",
      "Please uncomment the function(s) you want to run below.\n",
      "Loading saved model from models\\best_cvss_classifier_historic.pkl...\n",
      "Loading embedding model for prediction...\n",
      "\n",
      "Embedding Garak report for prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [11:22<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Predictions saved to data\\garak\\garak_with_severity_historic.csv\n",
      "\n",
      "--- Generating Final Vulnerability Score for the Report ---\n",
      "Severity Distribution:\n",
      " predicted_severity\n",
      "Medium      4504\n",
      "High        1555\n",
      "Critical      15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Raw Risk Score: 29051\n",
      "Normalized Report Vulnerability Score (0-100): 47.83\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import requests\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Text Embedding and ML Models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The classifiers we will compare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping\n",
    "\n",
    "# Import SMOTE for handling class imbalance\n",
    "# You may need to install this: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# --- Configuration for file paths ---\n",
    "# Base directories\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "\n",
    "# Subdirectories for data\n",
    "NVD_DATA_DIR = DATA_DIR / \"nvd_data\"\n",
    "GARAK_DATA_DIR = DATA_DIR / \"garak\"\n",
    "\n",
    "# Specific file paths\n",
    "PARSED_DATA_PATH = NVD_DATA_DIR / \"all_nvd_cves.pkl\"\n",
    "GARAK_REPORT_JSONL = GARAK_DATA_DIR / \"garak.report.jsonl\"\n",
    "GARAK_REPORT_CSV = GARAK_DATA_DIR / \"garak_report_flat.csv\"\n",
    "\n",
    "# Model file paths\n",
    "MODEL_PATH = MODELS_DIR / \"best_cvss_classifier_historic.pkl\"\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"cvss_label_encoder_historic.pkl\"\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1A: Process Garak Report\n",
    "# ----------------------------------------\n",
    "def process_garak_report():\n",
    "    \"\"\"\n",
    "    Downloads a sample Garak report if not present, and converts it\n",
    "    from .jsonl format to a flattened .csv file.\n",
    "    \"\"\"\n",
    "    # Create parent directories if they don't exist\n",
    "    GARAK_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download sample Garak report if not present\n",
    "    url = \"https://gist.githubusercontent.com/shubhobm/9fa52d71c8bb36bfb888eee2ba3d18f2/raw/ef1808e6d3b26002d9b046e6c120d438adf49008/gpt35-0906.report.jsonl\"\n",
    "    if not GARAK_REPORT_JSONL.exists():\n",
    "        print(\"Downloading sample Garak report...\")\n",
    "        urllib.request.urlretrieve(url, GARAK_REPORT_JSONL)\n",
    "        print(f\"âœ… Downloaded: {GARAK_REPORT_JSONL}\")\n",
    "\n",
    "    # Status decoding helper\n",
    "    def parse_status(status_code):\n",
    "        if status_code == 1:\n",
    "            return \"Pass\"\n",
    "        elif status_code == 2:\n",
    "            return \"Fail\"\n",
    "        else:\n",
    "            return \"Not Evaluated\"\n",
    "\n",
    "    # Turn-based or prompt-based format helper\n",
    "    def extract_input_output(record):\n",
    "        turns = record.get(\"notes\", {}).get(\"turns\", [])\n",
    "        if turns:  # Multi-turn conversation\n",
    "            attacker, bot = [], []\n",
    "            for role, msg in turns:\n",
    "                msg = msg.strip().replace(\"\\n\", \" \")\n",
    "                if role == \"probe\":\n",
    "                    attacker.append(msg)\n",
    "                elif role == \"model\":\n",
    "                    bot.append(msg)\n",
    "            return \" | \".join(attacker), \" | \".join(bot)\n",
    "\n",
    "        # Fallback to single-turn prompt + outputs\n",
    "        prompt = record.get(\"prompt\", \"\").strip().replace(\"\\n\", \" \")\n",
    "        outputs = record.get(\"outputs\", [])\n",
    "        output_texts = [o.strip().replace(\"\\n\", \" \") for o in outputs]\n",
    "        return prompt, \" | \".join(output_texts)\n",
    "\n",
    "    # Main conversion loop\n",
    "    with open(GARAK_REPORT_JSONL, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(GARAK_REPORT_CSV, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "        \n",
    "        writer = csv.DictWriter(outfile, fieldnames=[\n",
    "            \"uuid\", \"probe_classname\", \"attacker_input\", \"target_bot_response\", \n",
    "            \"status\", \"goal\", \"trigger\"\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            if record.get(\"entry_type\") != \"attempt\":\n",
    "                continue\n",
    "\n",
    "            writer.writerow({\n",
    "                \"uuid\": record.get(\"uuid\", \"\"),\n",
    "                \"probe_classname\": record.get(\"probe_classname\", \"\"),\n",
    "                \"attacker_input\": extract_input_output(record)[0],\n",
    "                \"target_bot_response\": extract_input_output(record)[1],\n",
    "                \"status\": parse_status(record.get(\"status\")),\n",
    "                \"goal\": record.get(\"goal\", \"\"),\n",
    "                \"trigger\": record.get(\"notes\", {}).get(\"trigger\", \"\")\n",
    "            })\n",
    "\n",
    "    print(f\"âœ… Garak report successfully converted to: {GARAK_REPORT_CSV}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1B: Download and Parse All Historical NVD Data\n",
    "# ----------------------------------------\n",
    "def download_and_parse_all_nvd_data():\n",
    "    \"\"\"\n",
    "    Downloads all NVD CVE data, parses them, removes duplicates, and saves\n",
    "    the result to a pickle file inside the nvd_data directory.\n",
    "    \"\"\"\n",
    "    # Create parent directories if they don't exist\n",
    "    NVD_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BASE_URL = \"https://nvd.nist.gov/feeds/json/cve/1.1/\"\n",
    "    START_YEAR = 2025\n",
    "    CURRENT_YEAR = datetime.now().year\n",
    "    \n",
    "    print(\"--- Starting NVD Data Download ---\")\n",
    "    for year in range(START_YEAR, CURRENT_YEAR + 1):\n",
    "        filename = f\"nvdcve-1.1-{year}.json.gz\"\n",
    "        download_path = NVD_DATA_DIR / filename\n",
    "        url = f\"{BASE_URL}{filename}\"\n",
    "\n",
    "        if download_path.exists():\n",
    "            print(f\"Skipping {filename}, already downloaded.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                with open(download_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                print(f\" -> Successfully saved to {download_path}\")\n",
    "            else:\n",
    "                print(f\" -> Failed to download {filename}: HTTP {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\" -> An error occurred while downloading {filename}: {e}\")\n",
    "    print(\"--- Download Process Complete ---\")\n",
    "\n",
    "    print(\"\\n--- Starting NVD Data Parsing ---\")\n",
    "    parsed_cve_list = []\n",
    "    for file_path in sorted(NVD_DATA_DIR.glob('*.json.gz')):\n",
    "        print(f\"Parsing: {file_path.name}\")\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                cve_data = json.load(f)\n",
    "            for item in cve_data.get(\"CVE_Items\", []):\n",
    "                description = next((d[\"value\"] for d in item.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", []) if d.get(\"lang\") == \"en\"), \"\")\n",
    "                impact = item.get(\"impact\", {})\n",
    "                severity = impact.get('baseMetricV3', {}).get('cvssV3', {}).get('baseSeverity') or impact.get('baseMetricV2', {}).get('severity')\n",
    "                \n",
    "                if description and severity:\n",
    "                    parsed_cve_list.append({\"description\": description.strip(), \"severity\": severity.strip().capitalize()})\n",
    "        except Exception as e:\n",
    "            print(f\" -> An error occurred while parsing {file_path.name}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(parsed_cve_list)\n",
    "    \n",
    "    print(\"\\n--- Removing Duplicates ---\")\n",
    "    print(f\"Number of entries before duplicate removal: {len(df)}\")\n",
    "    df.drop_duplicates(subset=['description'], keep='last', inplace=True)\n",
    "    print(f\"Number of entries after duplicate removal: {len(df)}\")\n",
    "    \n",
    "    df.to_pickle(PARSED_DATA_PATH)\n",
    "    print(f\"\\n--- Parsing Complete. Saved {len(df)} unique entries to {PARSED_DATA_PATH} ---\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 2: Find the Best Classifier and Train It\n",
    "# ----------------------------------------\n",
    "def train_and_evaluate_models():\n",
    "    if not PARSED_DATA_PATH.exists():\n",
    "        print(f\"Error: Parsed data not found at {PARSED_DATA_PATH}. Please run 'download' first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading parsed data from {PARSED_DATA_PATH}...\")\n",
    "    df = pd.read_pickle(PARSED_DATA_PATH)\n",
    "    df.dropna(subset=['description', 'severity'], inplace=True)\n",
    "    df = df[df['description'] != '']\n",
    "    print(f\"\\nTraining on {len(df)} valid NVD entries after cleaning.\")\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['severity'])\n",
    "    \n",
    "    print(\"Loading embedding model: 'all-MiniLM-L6-v2'...\")\n",
    "    embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    print(\"\\n!!! WARNING: Encoding all descriptions will take a very long time and consume significant memory. Please be patient. !!!\")\n",
    "    X = embed_model.encode(df['description'].tolist(), show_progress_bar=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # --- NEW: Apply SMOTE to handle class imbalance ---\n",
    "    print(\"\\nApplying SMOTE to balance the training data...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    print(\"SMOTE balancing complete. Training set size is now:\", X_train_resampled.shape)\n",
    "\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42), # No longer need class_weight='balanced'\n",
    "        \"Random Forest\": RandomForestClassifier(n_jobs=-1, random_state=42), # No longer need class_weight='balanced'\n",
    "        \"LightGBM (Tuned)\": lgb.LGBMClassifier(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "\n",
    "    best_f1, best_model_name, best_classifier_obj = -1, \"\", None\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "\n",
    "        # Use early stopping for LightGBM to find the best number of trees\n",
    "        if \"LightGBM\" in name:\n",
    "            clf.fit(X_train_resampled, y_train_resampled,\n",
    "                    eval_set=[(X_test, y_test)],\n",
    "                    eval_metric='multi_logloss',\n",
    "                    callbacks=[early_stopping(10, verbose=False)])\n",
    "        else:\n",
    "            # Train other models on the resampled data\n",
    "            clf.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        f1_score = report[\"weighted avg\"][\"f1-score\"]\n",
    "        if f1_score > best_f1:\n",
    "            best_f1, best_model_name, best_classifier_obj = f1_score, name, clf\n",
    "        print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "    print(f\"\\nðŸ† Best performing model is: {best_model_name}\")\n",
    "    print(f\"\\nRetraining {best_model_name} on the full resampled dataset for final model...\")\n",
    "    \n",
    "    # Retrain the final model on all data, resampled\n",
    "    X_resampled_full, y_resampled_full = smote.fit_resample(X, y)\n",
    "    best_classifier_obj.fit(X_resampled_full, y_resampled_full)\n",
    "\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(best_classifier_obj, MODEL_PATH)\n",
    "    joblib.dump(le, LABEL_ENCODER_PATH)\n",
    "    print(f\"âœ… Best model saved to {MODEL_PATH}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 3: Predict using a saved model\n",
    "# ----------------------------------------\n",
    "def predict_on_garak():\n",
    "    if not MODEL_PATH.exists() or not GARAK_REPORT_CSV.exists():\n",
    "        print(f\"Error: Model or Garak CSV not found. Please run 'train' and 'process_garak' first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading saved model from {MODEL_PATH}...\")\n",
    "    clf, le = joblib.load(MODEL_PATH), joblib.load(LABEL_ENCODER_PATH)\n",
    "    \n",
    "    print(\"Loading embedding model for prediction...\")\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    df = pd.read_csv(GARAK_REPORT_CSV)\n",
    "    df[\"full_text\"] = df[\"target_bot_response\"].fillna('')\n",
    "    \n",
    "    print(\"\\nEmbedding Garak report for prediction...\")\n",
    "    embeddings = model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    predicted_probabilities = clf.predict_proba(embeddings)\n",
    "    df[\"predicted_severity\"] = le.inverse_transform(np.argmax(predicted_probabilities, axis=1))\n",
    "    df[\"confidence_score\"] = np.round(np.max(predicted_probabilities, axis=1), 4)\n",
    "\n",
    "    for i, class_name in enumerate(le.classes_):\n",
    "        df[f'prob_{class_name.lower()}'] = np.round(predicted_probabilities[:, i], 4)\n",
    "\n",
    "    output_path = GARAK_DATA_DIR / \"garak_with_severity_historic.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to {output_path}\")\n",
    "\n",
    "    print(\"\\n--- Generating Final Vulnerability Score for the Report ---\")\n",
    "    severity_to_score = {'Critical': 10, 'High': 7, 'Medium': 4, 'Low': 1}\n",
    "    severity_counts = df['predicted_severity'].value_counts()\n",
    "    print(\"Severity Distribution:\\n\", severity_counts)\n",
    "    \n",
    "    total_score = sum(count * severity_to_score.get(s.capitalize(), 0) for s, count in severity_counts.items())\n",
    "    max_possible_score = len(df) * 10\n",
    "    normalized_score = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0\n",
    "\n",
    "    print(f\"\\nTotal Raw Risk Score: {total_score}\")\n",
    "    print(f\"Normalized Report Vulnerability Score (0-100): {normalized_score:.2f}\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# MAIN EXECUTION: Uncomment the action you want to run\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- CVE Severity Prediction Workflow ---\")\n",
    "    print(\"Please uncomment the function(s) you want to run below.\")\n",
    "\n",
    "    # # === ACTION 1: Download NVD data and process Garak report ===\n",
    "    # # Run this once to get all the data, or to update.\n",
    "    # process_garak_report()\n",
    "    # download_and_parse_all_nvd_data()\n",
    "    \n",
    "    # # === ACTION 2: Train a new model on the downloaded data ===\n",
    "    # # This is resource-intensive. Run it after downloading data.\n",
    "    # train_and_evaluate_models()\n",
    "\n",
    "    # === ACTION 3: Run prediction using the latest saved model ===\n",
    "    # This can be run anytime after a model has been trained and Garak report processed.\n",
    "    predict_on_garak()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
