--- CVE Severity Prediction Workflow ---

=== STAGE 1: DATA PREPARATION ===
Downloading sample Garak report...
âœ… Downloaded: data/4.0/garak/garak.report.jsonl
âœ… Garak report successfully converted to: data/4.0/garak/garak_report_flat.csv
--- Starting NVD Data Download ---
Downloading: nvdcve-1.1-2002.json.gz
Downloading: nvdcve-1.1-2003.json.gz
Downloading: nvdcve-1.1-2004.json.gz
Downloading: nvdcve-1.1-2005.json.gz
Downloading: nvdcve-1.1-2006.json.gz
Downloading: nvdcve-1.1-2007.json.gz
Downloading: nvdcve-1.1-2008.json.gz
Downloading: nvdcve-1.1-2009.json.gz
Downloading: nvdcve-1.1-2010.json.gz
Downloading: nvdcve-1.1-2011.json.gz
Downloading: nvdcve-1.1-2012.json.gz
Downloading: nvdcve-1.1-2013.json.gz
Downloading: nvdcve-1.1-2014.json.gz
Downloading: nvdcve-1.1-2015.json.gz
Downloading: nvdcve-1.1-2016.json.gz
Downloading: nvdcve-1.1-2017.json.gz
Downloading: nvdcve-1.1-2018.json.gz
Downloading: nvdcve-1.1-2019.json.gz
Downloading: nvdcve-1.1-2020.json.gz
Downloading: nvdcve-1.1-2021.json.gz
Downloading: nvdcve-1.1-2022.json.gz
Downloading: nvdcve-1.1-2023.json.gz
Downloading: nvdcve-1.1-2024.json.gz
Downloading: nvdcve-1.1-2025.json.gz

--- Starting NVD Data Parsing ---
Parsing nvdcve-1.1-2002.json.gz...
Parsing nvdcve-1.1-2003.json.gz...
Parsing nvdcve-1.1-2004.json.gz...
Parsing nvdcve-1.1-2005.json.gz...
Parsing nvdcve-1.1-2006.json.gz...
Parsing nvdcve-1.1-2007.json.gz...
Parsing nvdcve-1.1-2008.json.gz...
Parsing nvdcve-1.1-2009.json.gz...
Parsing nvdcve-1.1-2010.json.gz...
Parsing nvdcve-1.1-2011.json.gz...
Parsing nvdcve-1.1-2012.json.gz...
Parsing nvdcve-1.1-2013.json.gz...
Parsing nvdcve-1.1-2014.json.gz...
Parsing nvdcve-1.1-2015.json.gz...
Parsing nvdcve-1.1-2016.json.gz...
Parsing nvdcve-1.1-2017.json.gz...
Parsing nvdcve-1.1-2018.json.gz...
Parsing nvdcve-1.1-2019.json.gz...
Parsing nvdcve-1.1-2020.json.gz...
Parsing nvdcve-1.1-2021.json.gz...
Parsing nvdcve-1.1-2022.json.gz...
Parsing nvdcve-1.1-2023.json.gz...
Parsing nvdcve-1.1-2024.json.gz...
Parsing nvdcve-1.1-2025.json.gz...

Entries before duplicate removal: 155734
Entries after duplicate removal: 146631
âœ… Parsing Complete. Saved 146631 unique entries to data/4.0/nvd_data/all_nvd_cves.pkl

=== STAGE 2: MODEL TRAINING ===

--- Training and Evaluation with Undersampling ---
Loaded 146631 valid NVD entries.

Smallest class size is 2855. Undersampling all classes to this size.
Original dataset distribution:
 severity
Medium      64210
High        56874
Critical    22692
Low          2855
Name: count, dtype: int64

Balanced dataset distribution:
 severity
Low         2855
High        2855
Medium      2855
Critical    2855
Name: count, dtype: int64

Loading embedding model: 'all-MiniLM-L6-v2'...
Encoding all descriptions from the balanced dataset...
Batches:â€‡100%
â€‡357/357â€‡[12:13<00:00,â€‡â€‡3.31it/s]

Training on 8565 samples, testing on 2855 samples.

--- Training Logistic Regression ---
              precision    recall  f1-score   support

    Critical       0.64      0.73      0.68       714
        High       0.54      0.47      0.50       713
         Low       0.72      0.71      0.71       714
      Medium       0.64      0.65      0.65       714

    accuracy                           0.64      2855
   macro avg       0.64      0.64      0.64      2855
weighted avg       0.64      0.64      0.64      2855


--- Training Random Forest ---
              precision    recall  f1-score   support

    Critical       0.71      0.67      0.69       714
        High       0.58      0.51      0.54       713
         Low       0.65      0.79      0.72       714
      Medium       0.68      0.66      0.67       714

    accuracy                           0.66      2855
   macro avg       0.66      0.66      0.65      2855
weighted avg       0.66      0.66      0.65      2855


--- Training LightGBM (Tuned) ---
/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081990 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 97920
[LightGBM] [Info] Number of data points in the train set: 8565, number of used features: 384
[LightGBM] [Info] Start training from score -1.386411
[LightGBM] [Info] Start training from score -1.385944
[LightGBM] [Info] Start training from score -1.386411
[LightGBM] [Info] Start training from score -1.386411
/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
              precision    recall  f1-score   support

    Critical       0.71      0.75      0.73       714
        High       0.62      0.58      0.60       713
         Low       0.75      0.78      0.76       714
      Medium       0.71      0.70      0.70       714

    accuracy                           0.70      2855
   macro avg       0.70      0.70      0.70      2855
weighted avg       0.70      0.70      0.70      2855


ðŸ† Best performing model is: LightGBM (Tuned) with F1-Score: 0.6996
âœ… Best model saved to models/4.0/best_cvss_classifier_historic.pkl

=== STAGE 3: PREDICTION ===

--- Prediction on Garak Report ---
Embedding Garak report for prediction...
Batches:â€‡100%
â€‡190/190â€‡[13:27<00:00,â€‡â€‡1.43s/it]
/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
âœ… Predictions saved to data/4.0/garak/garak_with_severity_historic.csv

--- Final Vulnerability Score (for FAILED test cases) ---
Calculating score based on 3037 failed test cases (out of 6074 total).

Severity Distribution (of Failures):
predicted_severity
Low         2675
High         226
Medium       106
Critical      30
Name: count, dtype: int64

Total Raw Risk Score (from Failures): 4981
Normalized Report Vulnerability Score (0-100): 16.40

--- Workflow Finished ---