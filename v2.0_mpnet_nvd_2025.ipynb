{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufEda95T_0hM",
    "outputId": "ea255c57-993a-4753-a95a-e3326bb2339c"
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dg5rctvg-7DM",
    "outputId": "c25b1a7d-0624-4dce-ba74-ff4b583c5e16"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install sentence-transformers torch pandas scikit-learn joblib requests lightgbm torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3fa240ad884e41ffaa0e3203e2dfd112",
      "207ab08e013249ffa8ce845f5060afc5",
      "6dd0694302c648048ed9597c163170d2",
      "c6f1751de4284d1f94c07e74fa45d854",
      "759bcc0f75de413893b57859a0d5c9c8",
      "d85d0c83db1744ccb56372c2b7dec3e9",
      "be8bcce9a9e84ac49725fcaa6392126f",
      "674d9d7377f24582836a104b1765aa56",
      "c0eaa499bfa44ff380dac938947e6447",
      "67e0dcb5f57f42d08705329dd7a6beca",
      "053e50b9bb4843f6aadaabf3cc43700e",
      "16f352a46be6438d9edafa99d2b390fb",
      "82a37505e50145548862198432c9df40",
      "139fc1c20e4c4f6a97f13388e966d6a3",
      "80528e045b204d8082a56312051d261e",
      "d82afdc46a4b4f44b46dd4b207477852",
      "d42099fc4ee246d6abc3e8908953ba37",
      "34774f0015e242f0907e12b4e251e78f",
      "fb6b255486744783824ef0919ee2641d",
      "bbd8257af80a42bcb5ed2c4f6c3d2610",
      "cf691447e12543999da108759129cc69",
      "6c91239872cf4487b22d14ed5453a23f",
      "893baf8d9c48474baa550413c5202691",
      "afe611a69b45409fb32b922f6619b33b",
      "30d2604ec04541679c5000fb8f218349",
      "2bed500add374aa5af577a56d8ddfede",
      "5a9c5a06f33c4751bfa1f66887313d1b",
      "9bbd63ad3ab8403e8080cdadb544b04f",
      "1d9095871a8444548e39d60c32c6adc8",
      "56d8e4b0f75840a288df4ec949d28e4e",
      "68d4093ced9c4467b611ad072ca04998",
      "8e61a53b722041f49cd798431fee28bb",
      "ac757e30c7c2442fbfc926620a9e9566",
      "fe425aadab4e4465accbe77dae611146",
      "1cffb45ccf9040cbb234b912518a51d7",
      "a4534ebad26044c6aaad097bbab9117d",
      "68da1fff87e443939535d4f5e48e7c65",
      "a36572ea173e49c3959c8b0909d6082c",
      "911dddf4f82f449b90aa207a5d09d074",
      "f274692c39cf4c9693d5b8d465e88239",
      "1f28bcabae90412bae8c8e4c79c7d4b0",
      "3292b2274aba436da246632f6341b7a8",
      "bcc2bf38dcc84f1189ebe23c9f591c12",
      "f8b2329193f8416abe357d9cdb225cd8",
      "9845c908c1a44c0cae4eef2ae4280c12",
      "7134f3c1370e40e8a2f72c3a265678ef",
      "f858b4d847fe4a9aaddba15debe32d57",
      "61d5dad8f7db43f2a4de3dffee921333",
      "4cf2f8727d11430c8990f54cad540b43",
      "f27752cac5244c528c1358f580a3993f",
      "4401ffd4eb1d449cab4f9c46491d4994",
      "781dbf6a743b4412b1f47c18c4e890e3",
      "c1f40b2af55943f0a9d846c8bf22f4f3",
      "9228a0075aa1465098cbe1c588506e7a",
      "a685403bd8424023ab8ae7d1e2687aaa",
      "38028a3a057e42cdba88529c5e07ea9b",
      "5a15a51fa9984cd893816bb01fcede62",
      "ae5c9441fa774c548cdfbd7196cdf05c",
      "3ef1636f0455491a99d4893ac5fa5d17",
      "a97e357e635d4d3abb0e109324848157",
      "2a87d63a3e924e03b32181e0317bb087",
      "111727cd818c40d6bac369458bb217c2",
      "32fb1fbc66b34a7db17f5ece3d4a1de6",
      "26262205d6514caca2ff8f41e292e128",
      "82bfa6e382b34032b867529931b9c225",
      "0ec79a9beb6f4a938608c31071acca31",
      "f1cce69ff675419b873c533843d26d80",
      "3f2bc95fc17843eab66e29170baca0eb",
      "8fd25d23a70f4422b27761d2d31f2042",
      "4d2c4a84a3c14569835f54017e43394f",
      "fb66b8d2913241ad983f652c3713c58d",
      "10edbafd2db9499ca63827e8074a5c39",
      "b50fab3d04714df88cfc24bb1e943a77",
      "f263f41c33114c8a95ba8a82b50de29a",
      "e2320972435141a3823ab1c0115ffd19",
      "7ee378b93c01475b92ebc4997a625170",
      "352417f0b88a44d69dea9a3fface3f99",
      "fc1c660534374a4497665ed68bd624cc",
      "74564c8568ef4749a678aad9d0814cc3",
      "d41e827f893c409a8e1a53160baa8048",
      "ac01cafd89e449f7aa9b403680a47ff2",
      "e33ce125cd3345c5a71ddb5347112427",
      "bb30328b8e48449ea654b59b6abe3b0a",
      "623e9d695e6b43eeaa15e677938149c4",
      "56076915bb5c4c2289375a7226de93be",
      "fe67f91f82824364b4c5b921cd6dc0f8",
      "c3a77fc6b83f4ce78b5106adc6a8d37e",
      "0a76e35aa42a4bd28c81a8f25ad09765"
     ]
    },
    "id": "hpG6Ruv7--wH",
    "outputId": "d4fca5ca-a4a3-480e-8faa-aa8193e1e8ed"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import requests\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Text Embedding and ML Models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The classifiers we will compare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping\n",
    "\n",
    "# Import SMOTE for handling class imbalance\n",
    "# You may need to install this: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# --- Configuration for file paths ---\n",
    "# Base directories\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "VERSION = \"2.0\"\n",
    "\n",
    "\n",
    "# Subdirectories for data\n",
    "NVD_DATA_DIR = DATA_DIR / VERSION / \"nvd_data\"\n",
    "GARAK_DATA_DIR = DATA_DIR / VERSION / \"garak\"\n",
    "\n",
    "# Specific file paths\n",
    "PARSED_DATA_PATH = NVD_DATA_DIR / f\"all_nvd_cves.pkl\"\n",
    "GARAK_REPORT_JSONL = GARAK_DATA_DIR / \"garak.report.jsonl\"\n",
    "GARAK_REPORT_CSV = GARAK_DATA_DIR / \"garak_report_flat.csv\"\n",
    "\n",
    "# Model file paths\n",
    "MODEL_PATH = MODELS_DIR /VERSION/ \"best_cvss_classifier_historic.pkl\"\n",
    "LABEL_ENCODER_PATH = MODELS_DIR /VERSION /\"cvss_label_encoder_historic.pkl\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1A: Process Garak Report\n",
    "# ----------------------------------------\n",
    "def process_garak_report():\n",
    "    \"\"\"\n",
    "    Downloads a sample Garak report if not present, and converts it\n",
    "    from .jsonl format to a flattened .csv file.\n",
    "    \"\"\"\n",
    "    GARAK_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url = \"https://gist.githubusercontent.com/shubhobm/9fa52d71c8bb36bfb888eee2ba3d18f2/raw/ef1808e6d3b26002d9b046e6c120d438adf49008/gpt35-0906.report.jsonl\"\n",
    "    if not GARAK_REPORT_JSONL.exists():\n",
    "        print(\"Downloading sample Garak report...\")\n",
    "        urllib.request.urlretrieve(url, GARAK_REPORT_JSONL)\n",
    "        print(f\"âœ… Downloaded: {GARAK_REPORT_JSONL}\")\n",
    "\n",
    "    def parse_status(status_code):\n",
    "        return {1: \"Pass\", 2: \"Fail\"}.get(status_code, \"Not Evaluated\")\n",
    "\n",
    "    def extract_input_output(record):\n",
    "        turns = record.get(\"notes\", {}).get(\"turns\", [])\n",
    "        if turns:\n",
    "            attacker = \" | \".join([msg.strip().replace(\"\\n\", \" \") for role, msg in turns if role == \"probe\"])\n",
    "            bot = \" | \".join([msg.strip().replace(\"\\n\", \" \") for role, msg in turns if role == \"model\"])\n",
    "            return attacker, bot\n",
    "        prompt = record.get(\"prompt\", \"\").strip().replace(\"\\n\", \" \")\n",
    "        outputs = \" | \".join([o.strip().replace(\"\\n\", \" \") for o in record.get(\"outputs\", [])])\n",
    "        return prompt, outputs\n",
    "\n",
    "    with open(GARAK_REPORT_JSONL, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(GARAK_REPORT_CSV, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "\n",
    "        fieldnames = [\"uuid\", \"probe_classname\", \"attacker_input\", \"target_bot_response\", \"status\", \"goal\", \"trigger\"]\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            if record.get(\"entry_type\") != \"attempt\":\n",
    "                continue\n",
    "\n",
    "            attacker_input, bot_response = extract_input_output(record)\n",
    "            writer.writerow({\n",
    "                \"uuid\": record.get(\"uuid\", \"\"),\n",
    "                \"probe_classname\": record.get(\"probe_classname\", \"\"),\n",
    "                \"attacker_input\": attacker_input,\n",
    "                \"target_bot_response\": bot_response,\n",
    "                \"status\": parse_status(record.get(\"status\")),\n",
    "                \"goal\": record.get(\"goal\", \"\"),\n",
    "                \"trigger\": record.get(\"notes\", {}).get(\"trigger\", \"\")\n",
    "            })\n",
    "    print(f\"âœ… Garak report successfully converted to: {GARAK_REPORT_CSV}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1B: Download and Parse All Historical NVD Data\n",
    "# ----------------------------------------\n",
    "def download_and_parse_all_nvd_data():\n",
    "    \"\"\"\n",
    "    Downloads all NVD CVE data, parses them, removes duplicates, and saves\n",
    "    the result to a pickle file.\n",
    "    \"\"\"\n",
    "    NVD_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BASE_URL = \"https://nvd.nist.gov/feeds/json/cve/1.1/\"\n",
    "    START_YEAR, CURRENT_YEAR = 2025, datetime.now().year\n",
    "\n",
    "    print(\"--- Starting NVD Data Download ---\")\n",
    "    # This section remains the same as it's already optimized\n",
    "    for year in range(START_YEAR, CURRENT_YEAR + 1):\n",
    "        filename = f\"nvdcve-1.1-{year}.json.gz\"\n",
    "        download_path = NVD_DATA_DIR / filename\n",
    "        if download_path.exists(): continue\n",
    "        print(f\"Downloading: {filename}\")\n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{filename}\", stream=True, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                with open(download_path, 'wb') as f: f.writelines(response.iter_content(8192))\n",
    "            else: print(f\" -> Failed: HTTP {response.status_code}\")\n",
    "        except requests.RequestException as e: print(f\" -> Error: {e}\")\n",
    "\n",
    "    print(\"\\n--- Starting NVD Data Parsing ---\")\n",
    "    parsed_cve_list = []\n",
    "    for file_path in sorted(NVD_DATA_DIR.glob('*.json.gz')):\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            cve_data = json.load(f)\n",
    "        for item in cve_data.get(\"CVE_Items\", []):\n",
    "            description = next((d[\"value\"] for d in item.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", []) if d.get(\"lang\") == \"en\"), \"\")\n",
    "            impact = item.get(\"impact\", {})\n",
    "            severity = impact.get('baseMetricV3', {}).get('cvssV3', {}).get('baseSeverity') or impact.get('baseMetricV2', {}).get('severity')\n",
    "            if description and severity:\n",
    "                parsed_cve_list.append({\"description\": description.strip(), \"severity\": severity.strip().capitalize()})\n",
    "\n",
    "    df = pd.DataFrame(parsed_cve_list)\n",
    "    print(f\"\\nEntries before duplicate removal: {len(df)}\")\n",
    "    df.drop_duplicates(subset=['description'], keep='last', inplace=True)\n",
    "    print(f\"Entries after duplicate removal: {len(df)}\")\n",
    "    df.to_pickle(PARSED_DATA_PATH)\n",
    "    print(f\"âœ… Parsing Complete. Saved {len(df)} unique entries to {PARSED_DATA_PATH}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 2: Find the Best Classifier and Train It\n",
    "# ----------------------------------------\n",
    "def train_and_evaluate_models():\n",
    "    \"\"\"\n",
    "    Loads data, trains classifiers on a balanced dataset, finds the best one,\n",
    "    and saves it along with the label encoder.\n",
    "    \"\"\"\n",
    "    if not PARSED_DATA_PATH.exists():\n",
    "        print(f\"Error: Parsed data not found. Run data preparation first.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"\\n--- Training and Evaluation ---\")\n",
    "    df = pd.read_pickle(PARSED_DATA_PATH).dropna()\n",
    "    print(f\"Training on {len(df)} valid NVD entries.\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['severity'])\n",
    "\n",
    "    print(\"Loading embedding model: 'all-mpnet-base-v2'...\")\n",
    "    embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    print(\"\\nEncoding all descriptions... (This may take a while)\")\n",
    "    X = embed_model.encode(df['description'].tolist(), show_progress_bar=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "    print(\"\\nApplying SMOTE to balance the training data...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "        \"LightGBM (Tuned)\": lgb.LGBMClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            random_state=42,\n",
    "            feature_fraction=0.8,\n",
    "            bagging_fraction=0.8,\n",
    "            bagging_freq=1\n",
    "        )\n",
    "    }\n",
    "\n",
    "    best_f1, best_model_name, best_classifier_obj = -1, \"\", None\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        fit_params = {}\n",
    "        if \"LightGBM\" in name:\n",
    "            fit_params = {\"eval_set\": [(X_test, y_test)], \"callbacks\": [early_stopping(10, verbose=False)]}\n",
    "\n",
    "        clf.fit(X_train_resampled, y_train_resampled, **fit_params)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        f1_score = report[\"weighted avg\"][\"f1-score\"]\n",
    "        if f1_score > best_f1:\n",
    "            best_f1, best_model_name, best_classifier_obj = f1_score, name, clf\n",
    "        print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "    print(f\"\\nðŸ† Best performing model is: {best_model_name} with F1-Score: {best_f1:.4f}\")\n",
    "\n",
    "    # Save the best model that was trained and validated\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(best_classifier_obj, MODEL_PATH)\n",
    "    joblib.dump(le, LABEL_ENCODER_PATH)\n",
    "    print(f\"âœ… Best model saved to {MODEL_PATH}\")\n",
    "\n",
    "    return embed_model, best_classifier_obj, le\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 3: Predict using a saved model\n",
    "# ----------------------------------------\n",
    "def predict_on_garak(embed_model=None, classifier=None, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Loads a saved model or uses a passed one to predict severity on a Garak report.\n",
    "    \"\"\"\n",
    "    if not GARAK_REPORT_CSV.exists():\n",
    "        print(f\"Error: Garak CSV not found. Run data preparation first.\")\n",
    "        return\n",
    "\n",
    "    # Load models if they weren't passed from the training step\n",
    "    if not all([embed_model, classifier, label_encoder]):\n",
    "        print(f\"Loading models from disk...\")\n",
    "        if not MODEL_PATH.exists():\n",
    "            print(f\"Error: Model file not found at {MODEL_PATH}. Please train a model first.\")\n",
    "            return\n",
    "        classifier = joblib.load(MODEL_PATH)\n",
    "        label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "        embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    print(\"\\n--- Prediction on Garak Report ---\")\n",
    "    df = pd.read_csv(GARAK_REPORT_CSV)\n",
    "    df[\"full_text\"] = df[\"attacker_input\"].fillna('') + \" \" + df[\"target_bot_response\"].fillna('')\n",
    "\n",
    "    print(\"Embedding Garak report for prediction...\")\n",
    "    embeddings = embed_model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "    probabilities = classifier.predict_proba(embeddings)\n",
    "    df[\"predicted_severity\"] = label_encoder.inverse_transform(np.argmax(probabilities, axis=1))\n",
    "    df[\"confidence_score\"] = np.round(np.max(probabilities, axis=1), 4)\n",
    "    for i, name in enumerate(label_encoder.classes_):\n",
    "        df[f'prob_{name.lower()}'] = np.round(probabilities[:, i], 4)\n",
    "\n",
    "    output_path = GARAK_DATA_DIR / \"garak_with_severity_historic.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to {output_path}\")\n",
    "\n",
    "    # --- Generate Final Vulnerability Score for the Entire Report ---\n",
    "    print(\"\\n--- Final Vulnerability Score (for FAILED test cases) ---\")\n",
    "\n",
    "    # Filter for only failed test cases\n",
    "    failed_df = df[df['status'] == 'Fail'].copy()\n",
    "\n",
    "    if failed_df.empty:\n",
    "        print(\"No failed test cases found in the report. No score to calculate.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Calculating score based on {len(failed_df)} failed test cases (out of {len(df)} total).\")\n",
    "\n",
    "    # Define a mapping from severity to a numerical weight\n",
    "    severity_map = {'Critical': 10, 'High': 7, 'Medium': 4, 'Low': 1}\n",
    "\n",
    "    # Get the counts of each predicted severity from the FAILED cases\n",
    "    severity_counts = failed_df['predicted_severity'].value_counts()\n",
    "\n",
    "    # Calculate the final score by weighting the counts\n",
    "    total_score = sum(count * severity_map.get(s, 0) for s, count in severity_counts.items())\n",
    "\n",
    "    # Normalize the score based on the number of FAILED cases\n",
    "    max_possible_score = len(failed_df) * 10\n",
    "    normalized_score = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0\n",
    "\n",
    "    print(\"\\nSeverity Distribution (of Failures):\")\n",
    "    print(severity_counts)\n",
    "    print(f\"\\nTotal Raw Risk Score (from Failures): {total_score}\")\n",
    "    print(f\"Normalized Report Vulnerability Score (0-100): {normalized_score:.2f}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# MAIN EXECUTION WORKFLOW\n",
    "# ----------------------------------------\n",
    "def run_workflow(run_data_prep=False, run_training=False, run_prediction=False):\n",
    "    \"\"\"\n",
    "    Controls the main execution flow of the script.\n",
    "    \"\"\"\n",
    "    print(\"--- CVE Severity Prediction Workflow ---\")\n",
    "\n",
    "    if run_data_prep:\n",
    "        print(\"\\n=== STAGE 1: DATA PREPARATION ===\")\n",
    "        process_garak_report()\n",
    "        download_and_parse_all_nvd_data()\n",
    "\n",
    "    embed_model, classifier, label_encoder = None, None, None\n",
    "    if run_training:\n",
    "        print(\"\\n=== STAGE 2: MODEL TRAINING ===\")\n",
    "        embed_model, classifier, label_encoder = train_and_evaluate_models()\n",
    "\n",
    "    if run_prediction:\n",
    "        print(\"\\n=== STAGE 3: PREDICTION ===\")\n",
    "        # Pass the trained models to avoid re-loading if they exist\n",
    "        predict_on_garak(embed_model, classifier, label_encoder)\n",
    "\n",
    "    print(\"\\n--- Workflow Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configure your desired workflow here ---\n",
    "    # Set the flags to True for the steps you want to run.\n",
    "\n",
    "    # Example 1: Run the full pipeline from start to finish\n",
    "    run_workflow(run_data_prep=True, run_training=True, run_prediction=True)\n",
    "\n",
    "    # Example 2: Just run prediction using existing models\n",
    "    #run_workflow(run_prediction=True)\n",
    "\n",
    "    # Example 3: Prepare data and then train, but don't predict\n",
    "    # run_workflow(run_data_prep=True, run_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfPOz222C8KY",
    "outputId": "922d7f66-5455-425b-96cf-a9a3c68013e8"
   },
   "outputs": [],
   "source": [
    "!zip -r data.zip data/\n",
    "!zip -r models.zip models/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
