{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c965780b",
      "metadata": {
        "id": "c965780b"
      },
      "source": [
        "## Import Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e7846a",
      "metadata": {
        "id": "c1e7846a",
        "outputId": "ded01874-abcb-401b-8e5d-a54f7b2df6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "600ab2b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "600ab2b1",
        "outputId": "71961f72-673d-4fdd-a92f-d473a65d543a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install sentence-transformers torch pandas scikit-learn joblib requests lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import gzip\n",
        "import os\n",
        "import requests\n",
        "import joblib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import urllib.request\n",
        "\n",
        "# Text Embedding and ML Models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# The classifiers we will compare\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import lightgbm as lgb\n",
        "from lightgbm.callback import early_stopping\n",
        "\n",
        "# Import SMOTE for handling class imbalance\n",
        "# You may need to install this: pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# --- Configuration for file paths ---\n",
        "# Base directories\n",
        "DATA_DIR = Path(\"data\")\n",
        "MODELS_DIR = Path(\"models\")\n",
        "\n",
        "# Subdirectories for data\n",
        "NVD_DATA_DIR = DATA_DIR / \"nvd_data\"\n",
        "GARAK_DATA_DIR = DATA_DIR / \"garak\"\n",
        "\n",
        "# Specific file paths\n",
        "PARSED_DATA_PATH = NVD_DATA_DIR / \"all_nvd_cves.pkl\"\n",
        "GARAK_REPORT_JSONL = GARAK_DATA_DIR / \"garak.report.jsonl\"\n",
        "GARAK_REPORT_CSV = GARAK_DATA_DIR / \"garak_report_flat.csv\"\n",
        "\n",
        "# Model file paths\n",
        "MODEL_PATH = MODELS_DIR / \"best_cvss_classifier_historic.pkl\"\n",
        "LABEL_ENCODER_PATH = MODELS_DIR / \"cvss_label_encoder_historic.pkl\""
      ],
      "metadata": {
        "id": "ujvQVUOakGH7"
      },
      "id": "ujvQVUOakGH7",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Sample garak report for gpt35-0906.report.jsonl"
      ],
      "metadata": {
        "id": "AeReYUHQPpZ-"
      },
      "id": "AeReYUHQPpZ-"
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 1A: Process Garak Report\n",
        "# ----------------------------------------\n",
        "def process_garak_report():\n",
        "    \"\"\"\n",
        "    Downloads a sample Garak report if not present, and converts it\n",
        "    from .jsonl format to a flattened .csv file.\n",
        "    \"\"\"\n",
        "    # Create parent directories if they don't exist\n",
        "    GARAK_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download sample Garak report if not present\n",
        "    url = \"https://gist.githubusercontent.com/shubhobm/9fa52d71c8bb36bfb888eee2ba3d18f2/raw/ef1808e6d3b26002d9b046e6c120d438adf49008/gpt35-0906.report.jsonl\"\n",
        "    if not GARAK_REPORT_JSONL.exists():\n",
        "        print(\"Downloading sample Garak report...\")\n",
        "        urllib.request.urlretrieve(url, GARAK_REPORT_JSONL)\n",
        "        print(f\"✅ Downloaded: {GARAK_REPORT_JSONL}\")\n",
        "\n",
        "    # Status decoding helper\n",
        "    def parse_status(status_code):\n",
        "        if status_code == 1:\n",
        "            return \"Pass\"\n",
        "        elif status_code == 2:\n",
        "            return \"Fail\"\n",
        "        else:\n",
        "            return \"Not Evaluated\"\n",
        "\n",
        "    # Turn-based or prompt-based format helper\n",
        "    def extract_input_output(record):\n",
        "        turns = record.get(\"notes\", {}).get(\"turns\", [])\n",
        "        if turns:  # Multi-turn conversation\n",
        "            attacker, bot = [], []\n",
        "            for role, msg in turns:\n",
        "                msg = msg.strip().replace(\"\\n\", \" \")\n",
        "                if role == \"probe\":\n",
        "                    attacker.append(msg)\n",
        "                elif role == \"model\":\n",
        "                    bot.append(msg)\n",
        "            return \" | \".join(attacker), \" | \".join(bot)\n",
        "\n",
        "        # Fallback to single-turn prompt + outputs\n",
        "        prompt = record.get(\"prompt\", \"\").strip().replace(\"\\n\", \" \")\n",
        "        outputs = record.get(\"outputs\", [])\n",
        "        output_texts = [o.strip().replace(\"\\n\", \" \") for o in outputs]\n",
        "        return prompt, \" | \".join(output_texts)\n",
        "\n",
        "    # Main conversion loop\n",
        "    with open(GARAK_REPORT_JSONL, \"r\", encoding=\"utf-8\") as infile, \\\n",
        "         open(GARAK_REPORT_CSV, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
        "\n",
        "        writer = csv.DictWriter(outfile, fieldnames=[\n",
        "            \"uuid\", \"probe_classname\", \"attacker_input\", \"target_bot_response\",\n",
        "            \"status\", \"goal\", \"trigger\"\n",
        "        ])\n",
        "        writer.writeheader()\n",
        "\n",
        "        for line in infile:\n",
        "            record = json.loads(line)\n",
        "            if record.get(\"entry_type\") != \"attempt\":\n",
        "                continue\n",
        "\n",
        "            writer.writerow({\n",
        "                \"uuid\": record.get(\"uuid\", \"\"),\n",
        "                \"probe_classname\": record.get(\"probe_classname\", \"\"),\n",
        "                \"attacker_input\": extract_input_output(record)[0],\n",
        "                \"target_bot_response\": extract_input_output(record)[1],\n",
        "                \"status\": parse_status(record.get(\"status\")),\n",
        "                \"goal\": record.get(\"goal\", \"\"),\n",
        "                \"trigger\": record.get(\"notes\", {}).get(\"trigger\", \"\")\n",
        "            })\n",
        "\n",
        "    print(f\"✅ Garak report successfully converted to: {GARAK_REPORT_CSV}\")\n",
        "process_garak_report()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fOW1NehkMdu",
        "outputId": "965f952e-7b14-4d61-b180-df273ee7a6ff"
      },
      "id": "2fOW1NehkMdu",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sample Garak report...\n",
            "✅ Downloaded: data/garak/garak.report.jsonl\n",
            "✅ Garak report successfully converted to: data/garak/garak_report_flat.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007417f5",
      "metadata": {
        "id": "007417f5"
      },
      "source": [
        "## Download Latest cves data from NVD to train own CVSS seviarity score kind of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "999d019d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "999d019d",
        "outputId": "6d91c457-6e61-4a6b-efc3-1b9b491a7287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting NVD Data Download ---\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2002.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2002.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2003.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2003.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2004.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2004.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2005.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2005.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2006.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2006.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2007.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2007.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2008.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2008.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2009.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2009.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2010.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2010.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2011.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2011.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2012.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2012.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2013.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2013.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2014.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2014.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2015.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2015.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2016.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2016.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2017.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2017.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2018.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2018.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2019.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2019.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2020.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2021.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2022.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2022.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2023.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2023.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2024.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2024.json.gz\n",
            "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2025.json.gz\n",
            " -> Successfully saved to data/nvd_data/nvdcve-1.1-2025.json.gz\n",
            "--- Download Process Complete ---\n",
            "\n",
            "--- Starting NVD Data Parsing ---\n",
            "Parsing: nvdcve-1.1-2002.json.gz\n",
            "Parsing: nvdcve-1.1-2003.json.gz\n",
            "Parsing: nvdcve-1.1-2004.json.gz\n",
            "Parsing: nvdcve-1.1-2005.json.gz\n",
            "Parsing: nvdcve-1.1-2006.json.gz\n",
            "Parsing: nvdcve-1.1-2007.json.gz\n",
            "Parsing: nvdcve-1.1-2008.json.gz\n",
            "Parsing: nvdcve-1.1-2009.json.gz\n",
            "Parsing: nvdcve-1.1-2010.json.gz\n",
            "Parsing: nvdcve-1.1-2011.json.gz\n",
            "Parsing: nvdcve-1.1-2012.json.gz\n",
            "Parsing: nvdcve-1.1-2013.json.gz\n",
            "Parsing: nvdcve-1.1-2014.json.gz\n",
            "Parsing: nvdcve-1.1-2015.json.gz\n",
            "Parsing: nvdcve-1.1-2016.json.gz\n",
            "Parsing: nvdcve-1.1-2017.json.gz\n",
            "Parsing: nvdcve-1.1-2018.json.gz\n",
            "Parsing: nvdcve-1.1-2019.json.gz\n",
            "Parsing: nvdcve-1.1-2020.json.gz\n",
            "Parsing: nvdcve-1.1-2021.json.gz\n",
            "Parsing: nvdcve-1.1-2022.json.gz\n",
            "Parsing: nvdcve-1.1-2023.json.gz\n",
            "Parsing: nvdcve-1.1-2024.json.gz\n",
            "Parsing: nvdcve-1.1-2025.json.gz\n",
            "\n",
            "--- Removing Duplicates ---\n",
            "Number of entries before duplicate removal: 155734\n",
            "Number of entries after duplicate removal: 146631\n",
            "\n",
            "--- Parsing Complete. Saved 146631 unique entries to data/nvd_data/all_nvd_cves.pkl ---\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------\n",
        "# STEP 1B: Download and Parse All Historical NVD Data\n",
        "# ----------------------------------------\n",
        "def download_and_parse_all_nvd_data():\n",
        "    \"\"\"\n",
        "    Downloads all NVD CVE data, parses them, removes duplicates, and saves\n",
        "    the result to a pickle file inside the nvd_data directory.\n",
        "    \"\"\"\n",
        "    # Create parent directories if they don't exist\n",
        "    NVD_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    BASE_URL = \"https://nvd.nist.gov/feeds/json/cve/1.1/\"\n",
        "    START_YEAR = 2002\n",
        "    CURRENT_YEAR = datetime.now().year\n",
        "\n",
        "    print(\"--- Starting NVD Data Download ---\")\n",
        "    for year in range(START_YEAR, CURRENT_YEAR + 1):\n",
        "        filename = f\"nvdcve-1.1-{year}.json.gz\"\n",
        "        download_path = NVD_DATA_DIR / filename\n",
        "        url = f\"{BASE_URL}{filename}\"\n",
        "\n",
        "        if download_path.exists():\n",
        "            print(f\"Skipping {filename}, already downloaded.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading: {url}\")\n",
        "        try:\n",
        "            response = requests.get(url, stream=True, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                with open(download_path, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                print(f\" -> Successfully saved to {download_path}\")\n",
        "            else:\n",
        "                print(f\" -> Failed to download {filename}: HTTP {response.status_code}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\" -> An error occurred while downloading {filename}: {e}\")\n",
        "    print(\"--- Download Process Complete ---\")\n",
        "\n",
        "    print(\"\\n--- Starting NVD Data Parsing ---\")\n",
        "    parsed_cve_list = []\n",
        "    for file_path in sorted(NVD_DATA_DIR.glob('*.json.gz')):\n",
        "        print(f\"Parsing: {file_path.name}\")\n",
        "        try:\n",
        "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "                cve_data = json.load(f)\n",
        "            for item in cve_data.get(\"CVE_Items\", []):\n",
        "                description = next((d[\"value\"] for d in item.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", []) if d.get(\"lang\") == \"en\"), \"\")\n",
        "                impact = item.get(\"impact\", {})\n",
        "                severity = impact.get('baseMetricV3', {}).get('cvssV3', {}).get('baseSeverity') or impact.get('baseMetricV2', {}).get('severity')\n",
        "\n",
        "                if description and severity:\n",
        "                    parsed_cve_list.append({\"description\": description.strip(), \"severity\": severity.strip().capitalize()})\n",
        "        except Exception as e:\n",
        "            print(f\" -> An error occurred while parsing {file_path.name}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(parsed_cve_list)\n",
        "\n",
        "    print(\"\\n--- Removing Duplicates ---\")\n",
        "    print(f\"Number of entries before duplicate removal: {len(df)}\")\n",
        "    df.drop_duplicates(subset=['description'], keep='last', inplace=True)\n",
        "    print(f\"Number of entries after duplicate removal: {len(df)}\")\n",
        "\n",
        "    df.to_pickle(PARSED_DATA_PATH)\n",
        "    print(f\"\\n--- Parsing Complete. Saved {len(df)} unique entries to {PARSED_DATA_PATH} ---\")\n",
        "\n",
        "\n",
        "download_and_parse_all_nvd_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 2: Find the Best Classifier and Train It\n",
        "# ----------------------------------------\n",
        "def train_and_evaluate_models():\n",
        "    if not PARSED_DATA_PATH.exists():\n",
        "        print(f\"Error: Parsed data not found at {PARSED_DATA_PATH}. Please run 'download' first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading parsed data from {PARSED_DATA_PATH}...\")\n",
        "    df = pd.read_pickle(PARSED_DATA_PATH)\n",
        "    df.dropna(subset=['description', 'severity'], inplace=True)\n",
        "    df = df[df['description'] != '']\n",
        "    print(f\"\\nTraining on {len(df)} valid NVD entries after cleaning.\")\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(df['severity'])\n",
        "\n",
        "    print(\"Loading embedding model: 'all-mpnet-base-v2'...\")\n",
        "    embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    print(\"\\n!!! WARNING: Encoding all descriptions will take a very long time and consume significant memory. Please be patient. !!!\")\n",
        "    X = embed_model.encode(df['description'].tolist(), show_progress_bar=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # --- NEW: Apply SMOTE to handle class imbalance ---\n",
        "    print(\"\\nApplying SMOTE to balance the training data...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "    print(\"SMOTE balancing complete. Training set size is now:\", X_train_resampled.shape)\n",
        "\n",
        "    classifiers = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42), # No longer need class_weight='balanced'\n",
        "        \"Random Forest\": RandomForestClassifier(n_jobs=-1, random_state=42), # No longer need class_weight='balanced'\n",
        "        \"LightGBM (Tuned)\": lgb.LGBMClassifier(\n",
        "            n_estimators=1000,\n",
        "            learning_rate=0.05,\n",
        "            num_leaves=31,\n",
        "            random_state=42\n",
        "        )\n",
        "    }\n",
        "\n",
        "    best_f1, best_model_name, best_classifier_obj = -1, \"\", None\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\n--- Training {name} ---\")\n",
        "\n",
        "        # Use early stopping for LightGBM to find the best number of trees\n",
        "        if \"LightGBM\" in name:\n",
        "            clf.fit(X_train_resampled, y_train_resampled,\n",
        "                    eval_set=[(X_test, y_test)],\n",
        "                    eval_metric='multi_logloss',\n",
        "                    callbacks=[early_stopping(10, verbose=False)])\n",
        "        else:\n",
        "            # Train other models on the resampled data\n",
        "            clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "        f1_score = report[\"weighted avg\"][\"f1-score\"]\n",
        "        if f1_score > best_f1:\n",
        "            best_f1, best_model_name, best_classifier_obj = f1_score, name, clf\n",
        "        print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))\n",
        "\n",
        "    print(f\"\\n🏆 Best performing model is: {best_model_name}\")\n",
        "    print(f\"\\nRetraining {best_model_name} on the full resampled dataset for final model...\")\n",
        "\n",
        "    # Retrain the final model on all data, resampled\n",
        "    X_resampled_full, y_resampled_full = smote.fit_resample(X, y)\n",
        "    best_classifier_obj.fit(X_resampled_full, y_resampled_full)\n",
        "\n",
        "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    joblib.dump(best_classifier_obj, MODEL_PATH)\n",
        "    joblib.dump(le, LABEL_ENCODER_PATH)\n",
        "    print(f\"✅ Best model saved to {MODEL_PATH}\")\n",
        "\n",
        "\n",
        "train_and_evaluate_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b411655da98a403d972b94bcf7344c89",
            "e5baa3dcd9264c9894031b38343c23da",
            "360c3e5847cb4f4d9e397c21232fd563",
            "cb1b92e734184d769bbe74280bd26c29",
            "2dc3043d47604d9d874c9d28efdf092a",
            "a7bed485f3ea40ddbc488115f02e1261",
            "ddd29674c3394d0098b678179e40fbca",
            "4bdde54f794f47f5bec4761612335c48",
            "eea1b9c840f8480d89fd5f17b0a60acf",
            "0fb44e5ab0fd426788cec068bc43fdc4",
            "75863c1854504907bea024d57cb550cc"
          ]
        },
        "id": "G0Ih7n4Gkg8J",
        "outputId": "28695835-227d-4e8f-9426-5198245c6294"
      },
      "id": "G0Ih7n4Gkg8J",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading parsed data from data/nvd_data/all_nvd_cves.pkl...\n",
            "\n",
            "Training on 146631 valid NVD entries after cleaning.\n",
            "Loading embedding model: 'all-mpnet-base-v2'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "!!! WARNING: Encoding all descriptions will take a very long time and consume significant memory. Please be patient. !!!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b411655da98a403d972b94bcf7344c89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/4583 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Applying SMOTE to balance the training data...\n",
            "SMOTE balancing complete. Training set size is now: (192628, 768)\n",
            "\n",
            "--- Training Logistic Regression ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Critical       0.44      0.69      0.54      5673\n",
            "        High       0.64      0.48      0.55     14218\n",
            "         Low       0.08      0.65      0.15       714\n",
            "      Medium       0.79      0.56      0.66     16053\n",
            "\n",
            "    accuracy                           0.55     36658\n",
            "   macro avg       0.49      0.60      0.47     36658\n",
            "weighted avg       0.66      0.55      0.59     36658\n",
            "\n",
            "\n",
            "--- Training Random Forest ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Critical       0.56      0.54      0.55      5673\n",
            "        High       0.62      0.69      0.65     14218\n",
            "         Low       0.41      0.26      0.32       714\n",
            "      Medium       0.76      0.70      0.73     16053\n",
            "\n",
            "    accuracy                           0.66     36658\n",
            "   macro avg       0.59      0.55      0.56     36658\n",
            "weighted avg       0.67      0.66      0.66     36658\n",
            "\n",
            "\n",
            "--- Training LightGBM (Tuned) ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.834028 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 195840\n",
            "[LightGBM] [Info] Number of data points in the train set: 192628, number of used features: 768\n",
            "[LightGBM] [Info] Start training from score -1.386294\n",
            "[LightGBM] [Info] Start training from score -1.386294\n",
            "[LightGBM] [Info] Start training from score -1.386294\n",
            "[LightGBM] [Info] Start training from score -1.386294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# STEP 3: Predict using a saved model\n",
        "# ----------------------------------------\n",
        "def predict_on_garak():\n",
        "    if not MODEL_PATH.exists() or not GARAK_REPORT_CSV.exists():\n",
        "        print(f\"Error: Model or Garak CSV not found. Please run 'train' and 'process_garak' first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading saved model from {MODEL_PATH}...\")\n",
        "    clf, le = joblib.load(MODEL_PATH), joblib.load(LABEL_ENCODER_PATH)\n",
        "\n",
        "    print(\"Loading embedding model for prediction...\")\n",
        "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "    df = pd.read_csv(GARAK_REPORT_CSV)\n",
        "    df[\"full_text\"] = df[\"target_bot_response\"].fillna('')\n",
        "\n",
        "    print(\"\\nEmbedding Garak report for prediction...\")\n",
        "    embeddings = model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
        "\n",
        "    predicted_probabilities = clf.predict_proba(embeddings)\n",
        "    df[\"predicted_severity\"] = le.inverse_transform(np.argmax(predicted_probabilities, axis=1))\n",
        "    df[\"confidence_score\"] = np.round(np.max(predicted_probabilities, axis=1), 4)\n",
        "\n",
        "    for i, class_name in enumerate(le.classes_):\n",
        "        df[f'prob_{class_name.lower()}'] = np.round(predicted_probabilities[:, i], 4)\n",
        "\n",
        "    output_path = GARAK_DATA_DIR / \"garak_with_severity_historic.csv\"\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"✅ Predictions saved to {output_path}\")\n",
        "\n",
        "    print(\"\\n--- Generating Final Vulnerability Score for the Report ---\")\n",
        "    severity_to_score = {'Critical': 10, 'High': 7, 'Medium': 4, 'Low': 1}\n",
        "    severity_counts = df['predicted_severity'].value_counts()\n",
        "    print(\"Severity Distribution:\\n\", severity_counts)\n",
        "\n",
        "    total_score = sum(count * severity_to_score.get(s.capitalize(), 0) for s, count in severity_counts.items())\n",
        "    max_possible_score = len(df) * 10\n",
        "    normalized_score = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0\n",
        "\n",
        "    print(f\"\\nTotal Raw Risk Score: {total_score}\")\n",
        "    print(f\"Normalized Report Vulnerability Score (0-100): {normalized_score:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "predict_on_garak()\n"
      ],
      "metadata": {
        "id": "V3Ghxg8RlMCh"
      },
      "id": "V3Ghxg8RlMCh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b411655da98a403d972b94bcf7344c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5baa3dcd9264c9894031b38343c23da",
              "IPY_MODEL_360c3e5847cb4f4d9e397c21232fd563",
              "IPY_MODEL_cb1b92e734184d769bbe74280bd26c29"
            ],
            "layout": "IPY_MODEL_2dc3043d47604d9d874c9d28efdf092a"
          }
        },
        "e5baa3dcd9264c9894031b38343c23da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7bed485f3ea40ddbc488115f02e1261",
            "placeholder": "​",
            "style": "IPY_MODEL_ddd29674c3394d0098b678179e40fbca",
            "value": "Batches: 100%"
          }
        },
        "360c3e5847cb4f4d9e397c21232fd563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bdde54f794f47f5bec4761612335c48",
            "max": 4583,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eea1b9c840f8480d89fd5f17b0a60acf",
            "value": 4583
          }
        },
        "cb1b92e734184d769bbe74280bd26c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fb44e5ab0fd426788cec068bc43fdc4",
            "placeholder": "​",
            "style": "IPY_MODEL_75863c1854504907bea024d57cb550cc",
            "value": " 4583/4583 [17:27&lt;00:00, 23.17it/s]"
          }
        },
        "2dc3043d47604d9d874c9d28efdf092a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7bed485f3ea40ddbc488115f02e1261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddd29674c3394d0098b678179e40fbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bdde54f794f47f5bec4761612335c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eea1b9c840f8480d89fd5f17b0a60acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fb44e5ab0fd426788cec068bc43fdc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75863c1854504907bea024d57cb550cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}