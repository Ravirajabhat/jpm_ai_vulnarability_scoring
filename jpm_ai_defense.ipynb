{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c965780b",
   "metadata": {},
   "source": [
    "## Download Sample garak report for gpt35-0906.report.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ab2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers torch pandas scikit-learn joblib requests lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d0e36",
   "metadata": {},
   "source": [
    "## Download Sample garak report for gpt35-0906.report.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c289ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sample Garak report...\n",
      "✅ Downloaded: garak.report.jsonl\n",
      "✅ Garak report successfully converted to: garak_report_flat.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Download sample Garak report if not present\n",
    "url = \"https://gist.githubusercontent.com/shubhobm/9fa52d71c8bb36bfb888eee2ba3d18f2/raw/ef1808e6d3b26002d9b046e6c120d438adf49008/gpt35-0906.report.jsonl\"\n",
    "input_file = \"garak.report.jsonl\"\n",
    "if not os.path.exists(input_file):\n",
    "    print(\"Downloading sample Garak report...\")\n",
    "    urllib.request.urlretrieve(url, input_file)\n",
    "    print(\"✅ Downloaded:\", input_file)\n",
    "\n",
    "output_file = \"garak_report_flat.csv\"\n",
    "\n",
    "# Status decoding\n",
    "def parse_status(status_code):\n",
    "    if status_code == 1:\n",
    "        return \"Pass\"\n",
    "    elif status_code == 2:\n",
    "        return \"Fail\"\n",
    "    else:\n",
    "        return \"Not Evaluated\"\n",
    "\n",
    "# Parse turns-based or prompt-based format\n",
    "def extract_input_output(record):\n",
    "    turns = record.get(\"notes\", {}).get(\"turns\", [])\n",
    "    if turns:  # Multi-turn conversation\n",
    "        attacker, bot = [], []\n",
    "        for role, msg in turns:\n",
    "            msg = msg.strip().replace(\"\\n\", \" \")\n",
    "            if role == \"probe\":\n",
    "                attacker.append(msg)\n",
    "            elif role == \"model\":\n",
    "                bot.append(msg)\n",
    "        return \" | \".join(attacker), \" | \".join(bot)\n",
    "\n",
    "    # Fallback to single-turn prompt + outputs\n",
    "    prompt = record.get(\"prompt\", \"\").strip().replace(\"\\n\", \" \")\n",
    "    outputs = record.get(\"outputs\", [])\n",
    "    output_texts = [o.strip().replace(\"\\n\", \" \") for o in outputs]\n",
    "    return prompt, \" | \".join(output_texts)\n",
    "\n",
    "# Main conversion loop\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=[\n",
    "        \"uuid\", \"probe_classname\", \"attacker_input\", \"target_bot_response\", \"status\", \"goal\", \"trigger\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for line in infile:\n",
    "        record = json.loads(line)\n",
    "        if record.get(\"entry_type\") != \"attempt\":\n",
    "            continue\n",
    "\n",
    "        uuid = record.get(\"uuid\", \"\")\n",
    "        probe_class = record.get(\"probe_classname\", \"\")\n",
    "        status = parse_status(record.get(\"status\"))\n",
    "        attacker_input, bot_response = extract_input_output(record)\n",
    "        goal = record.get(\"goal\", \"\")\n",
    "        trigger = record.get(\"notes\", {}).get(\"trigger\", \"\")\n",
    "\n",
    "        writer.writerow({\n",
    "            \"uuid\": uuid,\n",
    "            \"probe_classname\": probe_class,\n",
    "            \"attacker_input\": attacker_input,\n",
    "            \"target_bot_response\": bot_response,\n",
    "            \"status\": status,\n",
    "            \"goal\": goal,\n",
    "            \"trigger\": trigger\n",
    "        })\n",
    "\n",
    "print(f\"✅ Garak report successfully converted to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007417f5",
   "metadata": {},
   "source": [
    "## Download Latest cves data from NVD to train own CVSS seviarity score Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CVE Severity Prediction Workflow ---\n",
      "Please uncomment the function(s) you want to run below.\n",
      "Downloading sample Garak report...\n",
      "✅ Downloaded: data\\garak\\garak.report.jsonl\n",
      "✅ Garak report successfully converted to: data\\garak\\garak_report_flat.csv\n",
      "--- Starting NVD Data Download ---\n",
      "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.gz\n",
      " -> Successfully saved to data\\nvd_data\\nvdcve-1.1-2020.json.gz\n",
      "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.gz\n",
      " -> Successfully saved to data\\nvd_data\\nvdcve-1.1-2021.json.gz\n",
      "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2022.json.gz\n",
      " -> Successfully saved to data\\nvd_data\\nvdcve-1.1-2022.json.gz\n",
      "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2023.json.gz\n",
      " -> Successfully saved to data\\nvd_data\\nvdcve-1.1-2023.json.gz\n",
      "Downloading: https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2024.json.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 289\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# === ACTION 1: Download NVD data and process Garak report ===\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Run this once to get all the data, or to update.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m process_garak_report()\n\u001b[1;32m--> 289\u001b[0m \u001b[43mdownload_and_parse_all_nvd_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# === ACTION 2: Train a new model on the downloaded data ===\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# This is resource-intensive. Run it after downloading data.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m train_and_evaluate_models()\n",
      "Cell \u001b[1;32mIn[3], line 147\u001b[0m, in \u001b[0;36mdownload_and_parse_all_nvd_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(download_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 147\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m):\n\u001b[0;32m    148\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m -> Successfully saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\nlp\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import requests\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Text Embedding and ML Models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The classifiers we will compare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Configuration for file paths ---\n",
    "# Base directories\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "\n",
    "# Subdirectories for data\n",
    "NVD_DATA_DIR = DATA_DIR / \"nvd_data\"\n",
    "GARAK_DATA_DIR = DATA_DIR / \"garak\"\n",
    "\n",
    "# Specific file paths\n",
    "PARSED_DATA_PATH = NVD_DATA_DIR / \"all_nvd_cves.pkl\"\n",
    "GARAK_REPORT_JSONL = GARAK_DATA_DIR / \"garak.report.jsonl\"\n",
    "GARAK_REPORT_CSV = GARAK_DATA_DIR / \"garak_report_flat.csv\"\n",
    "\n",
    "# Model file paths\n",
    "MODEL_PATH = MODELS_DIR / \"best_cvss_classifier_historic.pkl\"\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"cvss_label_encoder_historic.pkl\"\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1A: Process Garak Report\n",
    "# ----------------------------------------\n",
    "def process_garak_report():\n",
    "    \"\"\"\n",
    "    Downloads a sample Garak report if not present, and converts it\n",
    "    from .jsonl format to a flattened .csv file.\n",
    "    \"\"\"\n",
    "    # Create parent directories if they don't exist\n",
    "    GARAK_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download sample Garak report if not present\n",
    "    url = \"https://gist.githubusercontent.com/shubhobm/9fa52d71c8bb36bfb888eee2ba3d18f2/raw/ef1808e6d3b26002d9b046e6c120d438adf49008/gpt35-0906.report.jsonl\"\n",
    "    if not GARAK_REPORT_JSONL.exists():\n",
    "        print(\"Downloading sample Garak report...\")\n",
    "        urllib.request.urlretrieve(url, GARAK_REPORT_JSONL)\n",
    "        print(f\"✅ Downloaded: {GARAK_REPORT_JSONL}\")\n",
    "\n",
    "    # Status decoding helper\n",
    "    def parse_status(status_code):\n",
    "        if status_code == 1:\n",
    "            return \"Pass\"\n",
    "        elif status_code == 2:\n",
    "            return \"Fail\"\n",
    "        else:\n",
    "            return \"Not Evaluated\"\n",
    "\n",
    "    # Turn-based or prompt-based format helper\n",
    "    def extract_input_output(record):\n",
    "        turns = record.get(\"notes\", {}).get(\"turns\", [])\n",
    "        if turns:  # Multi-turn conversation\n",
    "            attacker, bot = [], []\n",
    "            for role, msg in turns:\n",
    "                msg = msg.strip().replace(\"\\n\", \" \")\n",
    "                if role == \"probe\":\n",
    "                    attacker.append(msg)\n",
    "                elif role == \"model\":\n",
    "                    bot.append(msg)\n",
    "            return \" | \".join(attacker), \" | \".join(bot)\n",
    "\n",
    "        # Fallback to single-turn prompt + outputs\n",
    "        prompt = record.get(\"prompt\", \"\").strip().replace(\"\\n\", \" \")\n",
    "        outputs = record.get(\"outputs\", [])\n",
    "        output_texts = [o.strip().replace(\"\\n\", \" \") for o in outputs]\n",
    "        return prompt, \" | \".join(output_texts)\n",
    "\n",
    "    # Main conversion loop\n",
    "    with open(GARAK_REPORT_JSONL, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(GARAK_REPORT_CSV, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "        \n",
    "        writer = csv.DictWriter(outfile, fieldnames=[\n",
    "            \"uuid\", \"probe_classname\", \"attacker_input\", \"target_bot_response\", \n",
    "            \"status\", \"goal\", \"trigger\"\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            if record.get(\"entry_type\") != \"attempt\":\n",
    "                continue\n",
    "\n",
    "            writer.writerow({\n",
    "                \"uuid\": record.get(\"uuid\", \"\"),\n",
    "                \"probe_classname\": record.get(\"probe_classname\", \"\"),\n",
    "                \"attacker_input\": extract_input_output(record)[0],\n",
    "                \"target_bot_response\": extract_input_output(record)[1],\n",
    "                \"status\": parse_status(record.get(\"status\")),\n",
    "                \"goal\": record.get(\"goal\", \"\"),\n",
    "                \"trigger\": record.get(\"notes\", {}).get(\"trigger\", \"\")\n",
    "            })\n",
    "\n",
    "    print(f\"✅ Garak report successfully converted to: {GARAK_REPORT_CSV}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1B: Download and Parse All Historical NVD Data\n",
    "# ----------------------------------------\n",
    "def download_and_parse_all_nvd_data():\n",
    "    \"\"\"\n",
    "    Downloads all NVD CVE data, parses them, removes duplicates, and saves\n",
    "    the result to a pickle file inside the nvd_data directory.\n",
    "    \"\"\"\n",
    "    # Create parent directories if they don't exist\n",
    "    NVD_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BASE_URL = \"https://nvd.nist.gov/feeds/json/cve/1.1/\"\n",
    "    START_YEAR = 2025\n",
    "    CURRENT_YEAR = datetime.now().year\n",
    "    \n",
    "    print(\"--- Starting NVD Data Download ---\")\n",
    "    for year in range(START_YEAR, CURRENT_YEAR + 1):\n",
    "        filename = f\"nvdcve-1.1-{year}.json.gz\"\n",
    "        download_path = NVD_DATA_DIR / filename\n",
    "        url = f\"{BASE_URL}{filename}\"\n",
    "\n",
    "        if download_path.exists():\n",
    "            print(f\"Skipping {filename}, already downloaded.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                with open(download_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                print(f\" -> Successfully saved to {download_path}\")\n",
    "            else:\n",
    "                print(f\" -> Failed to download {filename}: HTTP {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\" -> An error occurred while downloading {filename}: {e}\")\n",
    "    print(\"--- Download Process Complete ---\")\n",
    "\n",
    "    print(\"\\n--- Starting NVD Data Parsing ---\")\n",
    "    parsed_cve_list = []\n",
    "    for file_path in sorted(NVD_DATA_DIR.glob('*.json.gz')):\n",
    "        print(f\"Parsing: {file_path.name}\")\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                cve_data = json.load(f)\n",
    "            for item in cve_data.get(\"CVE_Items\", []):\n",
    "                description = next((d[\"value\"] for d in item.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", []) if d.get(\"lang\") == \"en\"), \"\")\n",
    "                impact = item.get(\"impact\", {})\n",
    "                severity = impact.get('baseMetricV3', {}).get('cvssV3', {}).get('baseSeverity') or impact.get('baseMetricV2', {}).get('severity')\n",
    "                \n",
    "                if description and severity:\n",
    "                    parsed_cve_list.append({\"description\": description.strip(), \"severity\": severity.strip().capitalize()})\n",
    "        except Exception as e:\n",
    "            print(f\" -> An error occurred while parsing {file_path.name}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(parsed_cve_list)\n",
    "    \n",
    "    print(\"\\n--- Removing Duplicates ---\")\n",
    "    print(f\"Number of entries before duplicate removal: {len(df)}\")\n",
    "    df.drop_duplicates(subset=['description'], keep='last', inplace=True)\n",
    "    print(f\"Number of entries after duplicate removal: {len(df)}\")\n",
    "    \n",
    "    df.to_pickle(PARSED_DATA_PATH)\n",
    "    print(f\"\\n--- Parsing Complete. Saved {len(df)} unique entries to {PARSED_DATA_PATH} ---\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 2: Find the Best Classifier and Train It\n",
    "# ----------------------------------------\n",
    "def train_and_evaluate_models():\n",
    "    if not PARSED_DATA_PATH.exists():\n",
    "        print(f\"Error: Parsed data not found at {PARSED_DATA_PATH}. Please run the 'download' command first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading parsed data from {PARSED_DATA_PATH}...\")\n",
    "    df = pd.read_pickle(PARSED_DATA_PATH)\n",
    "    df.dropna(subset=['description', 'severity'], inplace=True)\n",
    "    df = df[df['description'] != '']\n",
    "    print(f\"\\nTraining on {len(df)} valid NVD entries after cleaning.\")\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['severity'])\n",
    "    \n",
    "    print(\"Loading embedding model: 'all-mpnet-base-v2'...\")\n",
    "    embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    print(\"\\n!!! WARNING: Encoding all descriptions will take a very long time and consume significant memory. Please be patient. !!!\")\n",
    "    X = embed_model.encode(df['description'].tolist(), show_progress_bar=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42),\n",
    "        \"LightGBM\": lgb.LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        \"SVC\": SVC(class_weight='balanced', probability=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    best_f1, best_model_name, best_classifier_obj = -1, \"\", None\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        f1_score = report[\"weighted avg\"][\"f1-score\"]\n",
    "        if f1_score > best_f1:\n",
    "            best_f1, best_model_name, best_classifier_obj = f1_score, name, clf\n",
    "        print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "    print(f\"\\n🏆 Best performing model is: {best_model_name}\")\n",
    "    print(f\"\\nRetraining {best_model_name} on the full dataset...\")\n",
    "    best_classifier_obj.fit(X, y)\n",
    "\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(best_classifier_obj, MODEL_PATH)\n",
    "    joblib.dump(le, LABEL_ENCODER_PATH)\n",
    "    print(f\"✅ Best model saved to {MODEL_PATH}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 3: Predict using a saved model\n",
    "# ----------------------------------------\n",
    "def predict_on_garak():\n",
    "    if not MODEL_PATH.exists() or not GARAK_REPORT_CSV.exists():\n",
    "        print(f\"Error: Model or Garak CSV not found. Please run 'train' and 'process_garak' first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading saved model from {MODEL_PATH}...\")\n",
    "    clf, le = joblib.load(MODEL_PATH), joblib.load(LABEL_ENCODER_PATH)\n",
    "    \n",
    "    print(\"Loading embedding model for prediction...\")\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    df = pd.read_csv(GARAK_REPORT_CSV)\n",
    "    df[\"full_text\"] = df[\"attacker_input\"].fillna('') + \" \" + df[\"target_bot_response\"].fillna('')\n",
    "    \n",
    "    print(\"\\nEmbedding Garak report for prediction...\")\n",
    "    embeddings = model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    predicted_probabilities = clf.predict_proba(embeddings)\n",
    "    df[\"predicted_severity\"] = le.inverse_transform(np.argmax(predicted_probabilities, axis=1))\n",
    "    df[\"confidence_score\"] = np.round(np.max(predicted_probabilities, axis=1), 4)\n",
    "\n",
    "    for i, class_name in enumerate(le.classes_):\n",
    "        df[f'prob_{class_name.lower()}'] = np.round(predicted_probabilities[:, i], 4)\n",
    "\n",
    "    output_path = GARAK_DATA_DIR / \"garak_with_severity_historic.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Predictions saved to {output_path}\")\n",
    "\n",
    "    print(\"\\n--- Generating Final Vulnerability Score for the Report ---\")\n",
    "    severity_to_score = {'Critical': 10, 'High': 7, 'Medium': 4, 'Low': 1}\n",
    "    severity_counts = df['predicted_severity'].value_counts()\n",
    "    print(\"Severity Distribution:\\n\", severity_counts)\n",
    "    \n",
    "    total_score = sum(count * severity_to_score.get(s.capitalize(), 0) for s, count in severity_counts.items())\n",
    "    max_possible_score = len(df) * 10\n",
    "    normalized_score = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0\n",
    "\n",
    "    print(f\"\\nTotal Raw Risk Score: {total_score}\")\n",
    "    print(f\"Normalized Report Vulnerability Score (0-100): {normalized_score:.2f}\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# MAIN EXECUTION: Uncomment the action you want to run\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- CVE Severity Prediction Workflow ---\")\n",
    "    print(\"Please uncomment the function(s) you want to run below.\")\n",
    "\n",
    "    # === ACTION 1: Download NVD data and process Garak report ===\n",
    "    # Run this once to get all the data, or to update.\n",
    "    process_garak_report()\n",
    "    download_and_parse_all_nvd_data()\n",
    "    \n",
    "    # === ACTION 2: Train a new model on the downloaded data ===\n",
    "    # This is resource-intensive. Run it after downloading data.\n",
    "    train_and_evaluate_models()\n",
    "\n",
    "    # === ACTION 3: Run prediction using the latest saved model ===\n",
    "    # This can be run anytime after a model has been trained and Garak report processed.\n",
    "    predict_on_garak()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
